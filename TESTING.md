# ğŸ§ª A.R.C.A-LLM Testing Guide

GuÃ­a completa de testing para A.R.C.A-LLM Voice Assistant.

---

## ğŸ“Š Estado Actual

| CategorÃ­a | Tests | Status | Coverage |
|-----------|-------|--------|----------|
| **Domain (Message)** | 13 | âœ… 100% | 100% |
| **Domain (Conversation)** | 27 | âœ… 100% | 97% |
| **Application (ConversationService)** | 22 | âœ… 100% | 98% |
| **Integration (API)** | 14 | âœ… 100% | 84% (routes) |
| **TOTAL** | **76** | **âœ… 100%** | **Overall: 47%** |

**Notas**:
- 2 tests de integraciÃ³n se skip correctamente (requieren servicios reales)
- Infrastructure layer (STT/LLM/TTS) estÃ¡ mockeado en tests
- E2E tests pendientes (planeados para futuro)

---

## ğŸš€ EjecuciÃ³n RÃ¡pida

### Ejecutar todos los tests
```bash
python run_tests.py
```

### Tests por categorÃ­a
```bash
python run_tests.py --unit          # Solo tests unitarios (rÃ¡pido)
python run_tests.py --integration   # Solo tests de integraciÃ³n
python run_tests.py --domain        # Solo tests del dominio
python run_tests.py --application   # Solo tests de aplicaciÃ³n
```

### Con cobertura
```bash
python run_tests.py --coverage      # Reporte en terminal
python run_tests.py --html          # Genera HTML en htmlcov/
```

### Modo desarrollo
```bash
python run_tests.py --failed        # Solo tests que fallaron
python run_tests.py --watch         # Auto-reload (requiere pytest-watch)
```

---

## ğŸ“ Estructura de Tests

```
tests/
â”œâ”€â”€ conftest.py                  # Fixtures compartidos
â”œâ”€â”€ pytest.ini                   # ConfiguraciÃ³n de pytest
â”œâ”€â”€ README.md                    # DocumentaciÃ³n detallada
â”‚
â”œâ”€â”€ unit/                        # Tests unitarios (sin I/O)
â”‚   â”œâ”€â”€ domain/                 # Value Objects y Aggregates
â”‚   â”‚   â”œâ”€â”€ test_message.py    # âœ… 13 tests (100% coverage)
â”‚   â”‚   â””â”€â”€ test_conversation.py  # âœ… 27 tests (97% coverage)
â”‚   â”‚
â”‚   â”œâ”€â”€ application/            # Servicios de aplicaciÃ³n
â”‚   â”‚   â””â”€â”€ test_conversation_service.py  # âœ… 22 tests (98% coverage)
â”‚   â”‚
â”‚   â””â”€â”€ infrastructure/         # Clientes (mockeados)
â”‚       â””â”€â”€ (pendiente)
â”‚
â”œâ”€â”€ integration/                # Tests de API con mocks
â”‚   â””â”€â”€ test_api_endpoints.py  # âœ… 14 tests (84% coverage)
â”‚
â””â”€â”€ e2e/                        # End-to-end (futuro)
    â””â”€â”€ (pendiente)
```

---

## ğŸ¯ Estrategia de Testing

### 1. **Tests Unitarios** (`tests/unit/`)

**Objetivo**: Probar lÃ³gica de negocio aislada.

**CaracterÃ­sticas**:
- âœ… RÃ¡pidos (< 1 segundo por test)
- âœ… Sin I/O (sin red, sin disco, sin DB)
- âœ… Determin

Ã­sticos (mismos inputs = mismos outputs)
- âœ… FÃ¡ciles de depurar

**QuÃ© probamos**:
- **Domain Layer**: Value Objects, Entities, Aggregates
  - Inmutabilidad
  - Validaciones
  - LÃ³gica de negocio
  - Igualdad por valor
  
- **Application Layer**: Servicios de aplicaciÃ³n
  - OrquestaciÃ³n de domain objects
  - Casos de uso
  - CoordinaciÃ³n (sin lÃ³gica de negocio)

**Ejemplo**:
```python
def test_message_immutability():
    """Test that Message is immutable (frozen)."""
    msg = Message.create_user_message("Test")
    
    with pytest.raises(FrozenInstanceError):
        msg.content = "Modified"  # âŒ Debe fallar
```

---

### 2. **Tests de IntegraciÃ³n** (`tests/integration/`)

**Objetivo**: Probar integraciÃ³n entre capas.

**CaracterÃ­sticas**:
- âœ… Lentos (10-20 segundos)
- âœ… Con I/O mockeado (httpx AsyncClient)
- âœ… Prueban contratos entre capas

**QuÃ© probamos**:
- Endpoints de API
- ValidaciÃ³n de requests
- SerializaciÃ³n/deserializaciÃ³n
- CORS y headers
- Error handling

**Ejemplo**:
```python
async def test_health_endpoint(client):
    """Test /health endpoint returns status."""
    response = await client.get("/health")
    
    assert response.status_code == 200
    assert "status" in response.json()
```

---

### 3. **Tests E2E** (`tests/e2e/`) - Futuro

**Objetivo**: Probar flujos completos con servicios reales.

**CaracterÃ­sticas**:
- âš ï¸ Muy lentos (minutos)
- âš ï¸ Requieren servicios externos (LM Studio, Whisper)
- âš ï¸ FrÃ¡giles (dependen de red/servicios)

**QuÃ© probaremos**:
- ConversaciÃ³n completa STT â†’ LLM â†’ TTS
- Manejo de sesiones
- Performance real

---

## ğŸ§© Fixtures Disponibles

Ver `tests/conftest.py` para fixtures compartidos:

### Domain Fixtures
- `sample_conversation()` - ConversaciÃ³n de prueba
- `session_id()` - UUID Ãºnico
- `sample_messages()` - Lista de mensajes de ejemplo

### Application Fixtures
- `conversation_service()` - ConversationService real
- `voice_assistant_service()` - VoiceAssistantService mockeado

### Infrastructure Fixtures (Mocked)
- `mock_stt_client()` - WhisperSTTClient mockeado
- `mock_llm_client()` - LMStudioClient mockeado
- `mock_tts_client()` - Pyttsx3TTSClient mockeado

### Integration Fixtures
- `client()` - AsyncClient para tests de API
- `mock_voice_service_for_api()` - Voice service para API tests

---

## ğŸ“ˆ Coverage Reports

### Ver cobertura en terminal
```bash
python run_tests.py --coverage
```

### Generar HTML report
```bash
python run_tests.py --html
# Abre htmlcov/index.html en tu navegador
```

### Ver cobertura de un mÃ³dulo especÃ­fico
```bash
pytest tests/unit/domain/ --cov=src/domain --cov-report=term-missing
```

---

## ğŸ”§ ConfiguraciÃ³n

### pytest.ini

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

markers =
    unit: Unit tests (fast, no I/O)
    integration: Integration tests (with mocked I/O)
    e2e: End-to-end tests (with real services)
    slow: Slow tests (> 5 seconds)

asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

timeout = 300

# Coverage
addopts = 
    --strict-markers
    --cov=src
    --cov-report=term-missing
    --cov-report=html
    --cov-report=xml
    -ra
```

---

## ğŸ› Debugging Tests

### Ejecutar un test especÃ­fico
```bash
pytest tests/unit/domain/test_message.py::TestMessage::test_create_user_message -v
```

### Ver stdout/stderr
```bash
pytest tests/unit/domain/ -v -s
```

### Entrar en debugger cuando falla
```bash
pytest tests/unit/domain/ --pdb
```

### Ver logs
```bash
pytest tests/unit/domain/ -v --log-cli-level=DEBUG
```

---

## ğŸ“ Escribir Nuevos Tests

### Template para Test Unitario

```python
"""
Tests para [NombreDelMÃ³dulo].

Tests:
- [DescripciÃ³n del test]
- [DescripciÃ³n del test]
"""

import pytest
from src.domain.message import Message


class TestMessage:
    """Tests for Message Value Object."""
    
    def test_create_user_message(self):
        """Test that factory creates valid user message."""
        msg = Message.create_user_message("Hello")
        
        assert msg.role == "user"
        assert msg.content == "Hello"
        assert msg.timestamp is not None
```

### Template para Test de IntegraciÃ³n

```python
"""
Integration tests para [NombreDelEndpoint].

Tests:
- [DescripciÃ³n del test]
"""

import pytest
from httpx import AsyncClient


class TestMyEndpoint:
    """Tests for /my/endpoint."""
    
    @pytest.mark.asyncio
    @pytest.mark.integration
    async def test_my_endpoint(self, client):
        """Test endpoint returns expected response."""
        response = await client.get("/my/endpoint")
        
        assert response.status_code == 200
        assert "data" in response.json()
```

---

## ğŸš¨ CI/CD Integration (Futuro)

### GitHub Actions (ejemplo)

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
    
    - name: Run tests
      run: |
        python run_tests.py --coverage
    
    - name: Upload coverage
      uses: codecov/codecov-action@v2
```

---

## ğŸ“ Best Practices

### âœ… DO

- âœ… Escribir tests antes de refactorizar
- âœ… Un assert por test (cuando sea posible)
- âœ… Nombres descriptivos: `test_message_validates_empty_content()`
- âœ… Usar fixtures para setup repetitivo
- âœ… Tests independientes (no dependen entre sÃ­)
- âœ… Mockear I/O en tests unitarios
- âœ… Probar edge cases y errores

### âŒ DON'T

- âŒ Tests lentos en unit tests
- âŒ Tests que dependen de orden de ejecuciÃ³n
- âŒ Tests que modifican estado global
- âŒ MÃºltiples asserts no relacionados
- âŒ Tests que dependen de servicios externos (usar mocks)
- âŒ Tests sin asserts (smoke tests)
- âŒ Copy-paste de cÃ³digo de test

---

## ğŸ“š Recursos

### DocumentaciÃ³n
- [pytest](https://docs.pytest.org/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [httpx](https://www.python-httpx.org/async/)

### Testing Philosophy
- [Test Pyramid](https://martinfowler.com/articles/practical-test-pyramid.html)
- [Testing Best Practices](https://testdriven.io/blog/testing-best-practices/)
- [DDD Testing](https://www.domainlanguage.com/ddd/)

---

## ğŸ¤ Contribuir

Al agregar features nuevas:

1. âœ… Escribir tests primero (TDD)
2. âœ… Asegurar > 80% coverage
3. âœ… Ejecutar `python run_tests.py --coverage`
4. âœ… Verificar que todos los tests pasen
5. âœ… Commit con tests incluidos

---

## ğŸ“ Soporte

Â¿Problemas con los tests?

1. Revisar logs: `pytest -v --log-cli-level=DEBUG`
2. Revisar fixtures en `tests/conftest.py`
3. Revisar documentaciÃ³n en `tests/README.md`
4. Ejecutar tests en modo verbose: `pytest -vv`

---

**Ãšltima actualizaciÃ³n**: Noviembre 2025  
**VersiÃ³n**: 1.0.0  
**Mantenido por**: A.R.C.A-LLM Team

