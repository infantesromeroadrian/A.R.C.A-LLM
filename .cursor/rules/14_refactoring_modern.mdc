---
alwaysApply: true
---
## âš¡ **THE 3-STEP REFACTORING PROCESS**

### **Step 1: ğŸ” Analyze (30 seconds)**
```markdown
Before touching code:
â–¡ Read entire file to understand context
â–¡ Identify the worst code smell
â–¡ Plan ONE small improvement
â–¡ Ensure change won't break interfaces
```

### **Step 2: ğŸ› ï¸ Refactor (2-5 minutes)**
```markdown  
Make the change:
â–¡ Apply ONE technique at a time
â–¡ Use search_replace or MultiEdit
â–¡ Preserve existing functionality
â–¡ Add type hints and docstrings
```

### **Step 3: âœ… Verify (30 seconds)**
```markdown
Check the result:
â–¡ Imports still correct
â–¡ Function signatures compatible  
â–¡ Code more readable than before
â–¡ No new bugs introduced
```

---

## ğŸ› ï¸ **THE 6 ESSENTIAL REFACTORING TECHNIQUES**

### **1. ğŸ“ Extract Function (Most Common)**

When: Function > 20 lines or multiple responsibilities

```python
# âŒ BEFORE - One giant function
def train_ml_model(data, config):
    """Giant function doing everything."""
    
    # Data validation (8 lines)
    if data is None or data.empty:
        raise ValueError("Data cannot be empty")
    if 'target' not in data.columns:
        raise ValueError("Target column missing")
    # ... more validation
    
    # Data preprocessing (12 lines)
    from sklearn.preprocessing import StandardScaler
    scaler = StandardScaler()
    X = data.drop('target', axis=1)
    y = data['target']
    X_scaled = scaler.fit_transform(X)
    # ... more preprocessing
    
    # Model training (15 lines)
    from sklearn.ensemble import RandomForestClassifier
    model = RandomForestClassifier(**config)
    model.fit(X_scaled, y)
    # ... training logic
    
    # Model evaluation (10 lines)
    from sklearn.metrics import accuracy_score
    predictions = model.predict(X_scaled)
    accuracy = accuracy_score(y, predictions)
    # ... more evaluation
    
    return model, scaler, accuracy

# âœ… AFTER - Clean, focused functions
def validate_training_data(data: pd.DataFrame) -> None:
    """Validate data meets training requirements."""
    if data is None or data.empty:
        raise ValueError("Data cannot be empty")
    if 'target' not in data.columns:
        raise ValueError("Target column missing")

def preprocess_features(data: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, StandardScaler]:
    """Preprocess features for training."""
    X = data.drop('target', axis=1)
    y = data['target']
    
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    return X_scaled, y, scaler

def train_classifier(X: np.ndarray, y: np.ndarray, config: dict) -> RandomForestClassifier:
    """Train classification model with given config."""
    model = RandomForestClassifier(**config)
    model.fit(X, y)
    return model

def evaluate_model(model: Any, X: np.ndarray, y: np.ndarray) -> dict[str, float]:
    """Evaluate model performance."""
    predictions = model.predict(X)
    return {
        "accuracy": accuracy_score(y, predictions),
        "precision": precision_score(y, predictions, average='weighted'),
        "recall": recall_score(y, predictions, average='weighted')
    }

def train_ml_model(data: pd.DataFrame, config: dict) -> TrainingResult:
    """Train ML model with proper separation of concerns."""
    validate_training_data(data)
    X, y, scaler = preprocess_features(data)
    model = train_classifier(X, y, config)
    metrics = evaluate_model(model, X, y)
    
    return TrainingResult(model=model, scaler=scaler, metrics=metrics)
```

### **2. ğŸ·ï¸ Improve Names (Quick Win)**

When: Variables have unclear names

```python
# âŒ BEFORE - Cryptic names
def calc(d, m, t):
    if t == 'cls':
        return d * m
    return d

data = load_data()
res = calc(data, 0.8, 'cls')

# âœ… AFTER - Clear, descriptive names  
def calculate_classification_threshold(
    predictions: np.ndarray,
    multiplier: float,
    task_type: str
) -> float:
    """Calculate decision threshold for classification."""
    if task_type == 'classification':
        return predictions * multiplier
    return predictions

training_predictions = load_prediction_data()
decision_threshold = calculate_classification_threshold(
    training_predictions, 0.8, 'classification'
)
```

### **3. ğŸ”¢ Replace Magic Numbers**

When: Numbers appear without explanation

```python
# âŒ BEFORE - Magic numbers everywhere
class ModelTrainer:
    def train(self, data):
        if len(data) < 100:        # Magic number
            return None
        
        train_size = int(len(data) * 0.8)  # Magic number
        
        model = RandomForestClassifier(
            n_estimators=100,          # Magic number
            max_depth=10,              # Magic number
            min_samples_split=5        # Magic number
        )
        return model

# âœ… AFTER - Named constants with context
class ModelTrainer:
    """ML model trainer with configurable parameters."""
    
    # Data requirements
    MIN_TRAINING_SAMPLES = 100
    TRAIN_TEST_SPLIT_RATIO = 0.8
    
    # Model hyperparameters
    DEFAULT_N_ESTIMATORS = 100
    DEFAULT_MAX_DEPTH = 10
    DEFAULT_MIN_SAMPLES_SPLIT = 5
    
    def train(self, data: pd.DataFrame) -> Optional[RandomForestClassifier]:
        """Train model if sufficient data available."""
        if len(data) < self.MIN_TRAINING_SAMPLES:
            return None
        
        train_size = int(len(data) * self.TRAIN_TEST_SPLIT_RATIO)
        
        model = RandomForestClassifier(
            n_estimators=self.DEFAULT_N_ESTIMATORS,
            max_depth=self.DEFAULT_MAX_DEPTH,
            min_samples_split=self.DEFAULT_MIN_SAMPLES_SPLIT,
            random_state=42  # For reproducibility
        )
        return model
```

### **4. ğŸ§  Simplify Complex Conditions**

When: If statements are hard to understand

```python
# âŒ BEFORE - Complex condition nightmare
def should_retrain_model(model, metrics, data_drift, time_since_training):
    if model and metrics and metrics.get('accuracy', 0) < 0.85 and data_drift and data_drift.score > 0.3 and time_since_training and time_since_training.days > 30 and len(model.feature_names_in_) > 0:
        return True
    return False

# âœ… AFTER - Clear, readable conditions
def has_sufficient_accuracy(metrics: dict[str, float]) -> bool:
    """Check if model meets accuracy threshold."""
    return metrics.get('accuracy', 0) >= 0.85

def has_significant_data_drift(drift_score: Optional[float]) -> bool:
    """Check if data drift is significant enough to trigger retraining."""
    return drift_score is not None and drift_score > 0.3

def is_model_stale(days_since_training: int) -> bool:
    """Check if model is old enough to need retraining."""
    return days_since_training > 30

def is_model_valid(model: Any) -> bool:
    """Check if model is properly trained and usable."""
    return (model is not None and 
            hasattr(model, 'feature_names_in_') and 
            len(model.feature_names_in_) > 0)

def should_retrain_model(
    model: Any, 
    metrics: dict[str, float], 
    drift_score: Optional[float],
    days_since_training: int
) -> bool:
    """Determine if model should be retrained."""
    return (is_model_valid(model) and
            not has_sufficient_accuracy(metrics) and
            has_significant_data_drift(drift_score) and
            is_model_stale(days_since_training))
```

### **5. ğŸ”„ Eliminate Code Duplication**

When: Same pattern appears 2+ times

```python
# âŒ BEFORE - Duplicated ML preprocessing
def preprocess_training_data(data):
    data = data.dropna()
    data = data[data['amount'] > 0]  
    data = data[data['amount'] < 10000]
    data['log_amount'] = np.log(data['amount'])
    return data

def preprocess_validation_data(data):
    data = data.dropna()                      # Duplicate
    data = data[data['amount'] > 0]           # Duplicate
    data = data[data['amount'] < 10000]       # Duplicate  
    data['log_amount'] = np.log(data['amount'])  # Duplicate
    return data

def preprocess_test_data(data):
    data = data.dropna()                      # Duplicate
    data = data[data['amount'] > 0]           # Duplicate
    data = data[data['amount'] < 10000]       # Duplicate
    data['log_amount'] = np.log(data['amount'])  # Duplicate
    return data

# âœ… AFTER - DRY principle applied
def clean_financial_data(data: pd.DataFrame) -> pd.DataFrame:
    """Apply standard cleaning to financial data."""
    return (data
            .dropna()
            .query('amount > 0 and amount < 10000')
            .assign(log_amount=lambda df: np.log(df['amount'])))

def preprocess_training_data(data: pd.DataFrame) -> pd.DataFrame:
    """Preprocess training data."""
    return clean_financial_data(data)

def preprocess_validation_data(data: pd.DataFrame) -> pd.DataFrame:
    """Preprocess validation data."""
    return clean_financial_data(data)

def preprocess_test_data(data: pd.DataFrame) -> pd.DataFrame:
    """Preprocess test data."""
    return clean_financial_data(data)
```

### **6. âš ï¸ Improve Error Handling**

When: Generic or missing error handling

```python
# âŒ BEFORE - Poor error handling
def load_and_train_model(data_path, model_config):
    try:
        data = pd.read_csv(data_path)
        model = RandomForestClassifier(**model_config)
        model.fit(data.drop('target', axis=1), data['target'])
        return model
    except:  # Bare except - terrible!
        print("Something went wrong")
        return None

# âœ… AFTER - Specific, helpful error handling  
class ModelTrainingError(Exception):
    """Raised when model training fails."""
    pass

class DataLoadError(Exception):
    """Raised when data loading fails.""" 
    pass

def load_and_train_model(
    data_path: Path, 
    model_config: dict[str, Any]
) -> RandomForestClassifier:
    """
    Load data and train model with comprehensive error handling.
    
    Args:
        data_path: Path to training data CSV
        model_config: Model hyperparameters
        
    Returns:
        Trained RandomForest model
        
    Raises:
        DataLoadError: If data file cannot be loaded or is invalid
        ModelTrainingError: If model training fails
    """
    try:
        data = pd.read_csv(data_path)
    except FileNotFoundError:
        raise DataLoadError(f"Data file not found: {data_path}")
    except pd.errors.EmptyDataError:
        raise DataLoadError(f"Data file is empty: {data_path}")
    except Exception as e:
        raise DataLoadError(f"Failed to load data from {data_path}: {e}")
    
    # Validate data
    if data.empty:
        raise DataLoadError("Loaded data is empty")
    if 'target' not in data.columns:
        raise DataLoadError("Missing required 'target' column")
    
    try:
        X = data.drop('target', axis=1)
        y = data['target']
        
        model = RandomForestClassifier(**model_config)
        model.fit(X, y)
        
        return model
        
    except ValueError as e:
        raise ModelTrainingError(f"Invalid model configuration: {e}")
    except Exception as e:
        raise ModelTrainingError(f"Training failed: {e}")
```

---

## ğŸ¯ **REFACTORING PRIORITY MATRIX**

### **ğŸŸ¢ High Impact, Low Risk (Do First):**
- **Add type hints** â†’ No behavior change
- **Improve variable names** â†’ Easy to verify  
- **Add docstrings** â†’ Documentation only
- **Replace magic numbers** â†’ Clear improvements

### **ğŸŸ¡ Medium Impact, Medium Risk:**
- **Extract small functions** â†’ Verify logic preservation
- **Simplify conditions** â†’ Test thoroughly
- **Add error handling** â†’ Changes behavior but safer

### **ğŸ”´ High Risk (Do Carefully):**
- **Change function signatures** â†’ Could break callers
- **Extract classes** â†’ Major structural changes  
- **Refactor inheritance** â†’ Complex dependencies

---

## ğŸ“‹ **AI/ML SPECIFIC REFACTORING PATTERNS**

### **Data Pipeline Refactoring:**
```python
# âŒ BEFORE - Monolithic data pipeline
def process_ml_data(raw_data):
    # 50+ lines of mixed responsibilities
    cleaned = raw_data.dropna()
    scaled = StandardScaler().fit_transform(cleaned)
    encoded = LabelEncoder().fit_transform(cleaned['category'])
    # ... more processing
    return final_data

# âœ… AFTER - Pipeline pattern
class DataPipeline:
    """Composable data processing pipeline."""
    
    def __init__(self):
        self.steps = []
    
    def add_step(self, step: Callable[[pd.DataFrame], pd.DataFrame]) -> "DataPipeline":
        """Add processing step to pipeline."""
        self.steps.append(step)
        return self
    
    def process(self, data: pd.DataFrame) -> pd.DataFrame:
        """Run data through all pipeline steps."""
        result = data.copy()
        for step in self.steps:
            result = step(result)
        return result

# Usage - Clean and testable
pipeline = (DataPipeline()
    .add_step(remove_nulls)
    .add_step(scale_numeric_features)  
    .add_step(encode_categorical_features))

processed_data = pipeline.process(raw_data)
```

### **Model Factory Refactoring:**
```python
# âŒ BEFORE - Hardcoded model creation
def create_model(model_type, params):
    if model_type == "rf":
        return RandomForestClassifier(n_estimators=params.get('n_est', 100))
    elif model_type == "svm":
        return SVC(C=params.get('C', 1.0))
    # ... more hardcoded types

# âœ… AFTER - Extensible factory pattern
from typing import Protocol

class MLModel(Protocol):
    def fit(self, X, y): ...
    def predict(self, X): ...

class ModelFactory:
    """Factory for creating ML models."""
    
    _models: dict[str, type[MLModel]] = {}
    
    @classmethod
    def register(cls, name: str, model_class: type[MLModel]):
        """Register new model type."""
        cls._models[name] = model_class
    
    @classmethod  
    def create(cls, model_type: str, **params) -> MLModel:
        """Create model instance."""
        if model_type not in cls._models:
            available = list(cls._models.keys())
            raise ValueError(f"Unknown model: {model_type}. Available: {available}")
        
        model_class = cls._models[model_type]
        return model_class(**params)

# Register models
ModelFactory.register("random_forest", RandomForestClassifier)
ModelFactory.register("svm", SVC)

# Usage - Clean and extensible
model = ModelFactory.create("random_forest", n_estimators=100, random_state=42)
```

---

## âœ… **QUICK REFACTORING CHECKLIST**

**Before starting (30s):**
- [ ] **Understand context** - Read entire file
- [ ] **Identify worst smell** - Pick one issue to fix
- [ ] **Plan small change** - One technique at a time

**During refactoring (2-5m):**
- [ ] **Edit existing file** - Never create new files
- [ ] **Preserve functionality** - Same inputs â†’ same outputs
- [ ] **Add type hints** - Modern Python practices
- [ ] **Add docstrings** - Document new functions

**After refactoring (30s):**
- [ ] **Check imports** - Still correct after changes
- [ ] **Verify interfaces** - No breaking changes
- [ ] **Test mentally** - Logic still sound
- [ ] **Readability improved** - Code clearer than before

---

## ğŸ’¡ **REFACTORING MANTRAS**

> **"One file, one change, one technique at a time"**

> **"Preserve behavior, improve structure"**

> **"Better names solve 80% of clarity problems"**

> **"Small functions are testable functions"**

> **"When in doubt, extract it out"**

---

## ğŸ† **SUCCESS METRICS**

A successful refactoring achieves:

âœ… **Same functionality** - All behavior preserved  
âœ… **Better readability** - Code easier to understand
âœ… **Reduced complexity** - Functions under 20 lines
âœ… **Clear names** - No cryptic variables
âœ… **Proper error handling** - Specific exceptions
âœ… **Type safety** - Complete type hints
âœ… **DRY principle** - No code duplication

**Remember: The best refactoring is invisible to users but obvious to developers!** ğŸš€
