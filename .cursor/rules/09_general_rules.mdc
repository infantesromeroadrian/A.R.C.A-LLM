---
description: ML Best Practices - Security, Performance, Testing, and CI/CD for production-ready ML systems
globs: 
alwaysApply: true
---

# ðŸš€ ML Best Practices (Security, Performance, Testing, CI/CD)

## ðŸŽ¯ **Philosophy: Production-Ready ML Systems**

> *"Good ML engineering prevents problems before they happen in production."*

---

## ðŸ—ï¸ **THE 4 ESSENTIAL PILLARS**

### **1. ðŸ”’ Security (ML-Specific Protection)**

Protect your AI systems from real-world threats.

```python
# âœ… MODERN - Environment-based configuration
import os
from pydantic_settings import BaseSettings
from typing import Literal

class Settings(BaseSettings):
    """Secure settings with validation."""
  
    # Database
    database_url: str
  
    # AI Model Settings
    model_name: str = "gpt-3.5-turbo"
    openai_api_key: str
    max_tokens: int = 1000
  
    # Security
    allowed_origins: list[str] = ["http://localhost:3000"]
    rate_limit_per_minute: int = 60
    environment: Literal["dev", "staging", "prod"] = "dev"
  
    class Config:
        env_file = ".env"

settings = Settings()

# âœ… MODERN - Input validation for ML APIs
from pydantic import BaseModel, Field, validator
from fastapi import HTTPException

class MLPredictionRequest(BaseModel):
    """Validated ML prediction request."""
  
    text: str = Field(..., min_length=1, max_length=5000)
    model_name: str = Field(..., regex=r'^[a-zA-Z0-9_-]+$')
    confidence_threshold: float = Field(0.5, ge=0.0, le=1.0)
  
    @validator('text')
    def sanitize_text(cls, v: str) -> str:
        """Clean and validate input text."""
        # Remove potential injection patterns
        dangerous_patterns = ['<script>', 'javascript:', 'eval(', 'exec(']
        for pattern in dangerous_patterns:
            if pattern.lower() in v.lower():
                raise ValueError("Potentially dangerous input detected")
      
        return v.strip()

# âœ… MODERN - Rate limiting for ML APIs
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from fastapi import Request

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

@app.post("/predict")
@limiter.limit("10/minute")  # Prevent ML API abuse
async def predict(request: Request, data: MLPredictionRequest):
    """Secure ML prediction endpoint."""
    try:
        # Validate input size to prevent memory attacks
        if len(data.text.encode('utf-8')) > 1024 * 1024:  # 1MB limit
            raise HTTPException(status_code=413, detail="Input too large")
      
        result = await prediction_service.predict(data)
        return {"prediction": result, "confidence": result.confidence}
      
    except Exception as e:
        # Don't leak internal errors
        logger.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail="Prediction failed")

# âŒ BAD - Security vulnerabilities
@app.post("/predict")  
async def bad_predict(text: str, model: str):
    # No input validation - injection risk!
    model_path = f"/models/{model}.pkl"  # Path traversal vulnerability!
    loaded_model = pickle.load(open(model_path, 'rb'))  # Pickle vulnerability!
    return eval(f"loaded_model.predict('{text}')")  # Code injection!
```

---

### **2. âš¡ Performance (Handle Real Data Sizes)**

Build for production-scale data, not toy datasets.

```python
# âœ… MODERN - Async data processing
import asyncio
import aiohttp
import aiofiles
from concurrent.futures import ThreadPoolExecutor

class AsyncMLPipeline:
    """High-performance ML pipeline with async processing."""
  
    def __init__(self, max_workers: int = 4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
  
    async def process_large_dataset(self, file_paths: list[Path]) -> list[pd.DataFrame]:
        """Process multiple files concurrently."""
      
        async def process_single_file(path: Path) -> pd.DataFrame:
            # Run CPU-bound work in thread pool
            loop = asyncio.get_event_loop()
            return await loop.run_in_executor(
                self.executor, 
                self._process_file_sync, 
                path
            )
      
        # Process all files concurrently
        tasks = [process_single_file(path) for path in file_paths]
        return await asyncio.gather(*tasks)
  
    def _process_file_sync(self, path: Path) -> pd.DataFrame:
        """Synchronous file processing (runs in thread pool)."""
        return pd.read_csv(path).pipe(self._clean_data)

# âœ… MODERN - Memory-efficient batch processing
from typing import Iterator

def process_huge_dataset(
    data_path: Path, 
    batch_size: int = 10000,
    model: Any = None
) -> Iterator[dict[str, Any]]:
    """Process datasets too large for memory."""
  
    total_processed = 0
  
    # Process in chunks to manage memory
    for chunk in pd.read_csv(data_path, chunksize=batch_size):
        # Clean chunk
        cleaned_chunk = chunk.dropna().select_dtypes(include=[np.number])
      
        # Process with model if provided
        if model is not None:
            predictions = model.predict(cleaned_chunk)
          
            yield {
                "batch_size": len(cleaned_chunk),
                "predictions": predictions.tolist(),
                "batch_number": total_processed // batch_size + 1
            }
      
        total_processed += len(chunk)
      
        # Explicit memory management
        del chunk, cleaned_chunk
      
        # Log progress
        if total_processed % 50000 == 0:
            print(f"Processed {total_processed:,} rows")

# âœ… MODERN - Smart caching for expensive operations
from functools import lru_cache
import joblib
from pathlib import Path

class CachedModelPredictor:
    """Model predictor with intelligent caching."""
  
    def __init__(self, model_path: Path):
        self.model_path = model_path
        self._model = None
  
    @property
    def model(self):
        """Lazy load model only when needed."""
        if self._model is None:
            self._model = joblib.load(self.model_path)
        return self._model
  
    @lru_cache(maxsize=1000)
    def _compute_features(self, text: str) -> tuple[float, ...]:
        """Cache expensive feature computation."""
        # This would be expensive feature extraction
        features = self._extract_features(text)
        return tuple(features)  # Must be hashable for cache
  
    async def predict_batch(self, texts: list[str]) -> list[float]:
        """Efficient batch prediction with caching."""
        loop = asyncio.get_event_loop()
      
        # Compute features (with caching)
        feature_tasks = [
            loop.run_in_executor(None, self._compute_features, text)
            for text in texts
        ]
        feature_vectors = await asyncio.gather(*feature_tasks)
      
        # Batch prediction
        X = np.array(feature_vectors)
        predictions = await loop.run_in_executor(None, self.model.predict, X)
      
        return predictions.tolist()

# âŒ BAD - Performance killers
def bad_performance():
    # Loading entire dataset into memory
    huge_df = pd.read_csv("100GB_dataset.csv")  # Memory explosion!
  
    # No caching of expensive operations
    for text in texts:
        expensive_features = extract_features(text)  # Recalculated every time!
  
    # Synchronous I/O blocking
    for url in urls:
        response = requests.get(url)  # Blocks entire pipeline!
```

---

### **3. ðŸ§ª Testing (ML-Specific Testing)**

Test your models, not just your functions.

```python
# âœ… MODERN - Property-based testing for ML
from hypothesis import given, strategies as st
import hypothesis.extra.numpy as hnp

class TestMLModel:
    """Comprehensive ML model testing."""
  
    @pytest.fixture
    def trained_model(self):
        """Provide consistent trained model for tests."""
        X, y = make_classification(n_samples=1000, random_state=42)
        model = RandomForestClassifier(random_state=42)
        return model.fit(X, y)
  
    @given(
        data=hnp.arrays(
            dtype=np.float32,
            shape=hnp.array_shapes(min_dims=2, max_dims=2),
            elements=st.floats(min_value=-10, max_value=10, allow_nan=False)
        )
    )
    def test_model_prediction_properties(self, trained_model, data):
        """Test that model predictions have expected properties."""
        # Skip if data is too small
        assume(data.shape[0] > 0 and data.shape[1] == trained_model.n_features_in_)
      
        predictions = trained_model.predict(data)
      
        # Properties that should always hold
        assert len(predictions) == len(data)
        assert all(isinstance(p, (int, np.integer)) for p in predictions)
        assert all(p in trained_model.classes_ for p in predictions)
  
    def test_model_reproducibility(self):
        """Test that model training is reproducible."""
        X, y = make_classification(n_samples=100, random_state=42)
      
        # Train two identical models
        model1 = RandomForestClassifier(random_state=42)
        model2 = RandomForestClassifier(random_state=42)
      
        model1.fit(X, y)
        model2.fit(X, y)
      
        # Predictions should be identical
        test_data = np.random.rand(10, X.shape[1])
        pred1 = model1.predict(test_data)
        pred2 = model2.predict(test_data)
      
        np.testing.assert_array_equal(pred1, pred2)
  
    def test_model_performance_benchmarks(self, trained_model):
        """Test that model meets performance requirements."""
        X_test, y_test = make_classification(n_samples=200, random_state=123)
      
        predictions = trained_model.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
      
        # Model should meet minimum performance threshold
        assert accuracy >= 0.8, f"Model accuracy {accuracy:.3f} below threshold"
  
    @pytest.mark.asyncio
    async def test_async_prediction_performance(self):
        """Test async prediction performance."""
        predictor = AsyncMLPredictor()
        test_texts = ["sample text"] * 100
      
        start_time = time.time()
        results = await predictor.predict_batch(test_texts)
        end_time = time.time()
      
        # Should process 100 items in under 5 seconds
        assert end_time - start_time < 5.0
        assert len(results) == 100

# âœ… MODERN - Data quality testing
def test_data_quality():
    """Test data meets quality requirements."""
    data = load_training_data()
  
    # Data completeness
    assert data.isnull().sum().sum() < len(data) * 0.05, "Too many missing values"
  
    # Data distribution
    assert data['target'].value_counts().min() > 100, "Class imbalance too severe"
  
    # Feature quality
    numeric_cols = data.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        assert not np.isinf(data[col]).any(), f"Infinite values in {col}"
        assert abs(data[col].skew()) < 5, f"Extreme skewness in {col}"

# âŒ BAD - Meaningless tests
def test_model():
    model = SomeModel()
    assert model is not None  # Tells us nothing!

def test_prediction():
    result = model.predict([1, 2, 3])
    assert result  # What does this even test?
```

---

### **4. ðŸ”„ Workflow (Git + CI/CD for ML)**

Streamlined workflow that doesn't slow you down.

```yaml
# .github/workflows/ml-pipeline.yml - Modern CI/CD for ML
name: ML Pipeline
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
    
      # Fast Python setup
      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
        
      # Cache dependencies for speed
      - uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('requirements.txt') }}
    
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install ruff pytest-cov
    
      # Fast linting with ruff (replaces flake8, black, isort)
      - name: Lint with ruff
        run: ruff check src/ tests/
    
      # Type checking
      - name: Type check with mypy
        run: mypy src/ --strict
    
      # Run tests with coverage
      - name: Test with pytest
        run: |
          pytest tests/ \
            --cov=src \
            --cov-report=xml \
            --cov-fail-under=90
    
      # ML-specific checks
      - name: Check for data leakage
        run: python scripts/check_data_leakage.py
      
      - name: Validate model interfaces
        run: python scripts/validate_models.py

  deploy:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
  
    steps:
      - uses: actions/checkout@v4
    
      - name: Deploy to staging
        run: |
          docker build -t ml-app .
          docker tag ml-app registry.example.com/ml-app:latest
          docker push registry.example.com/ml-app:latest
```

**Modern Git Workflow:**

```bash
# âœ… GOOD - Semantic commit messages
git commit -m "feat: add async batch prediction endpoint

- Implement AsyncMLPredictor for high-throughput inference
- Add input validation and rate limiting
- Include performance benchmarks in tests
- Update API documentation

Closes #123"

git commit -m "fix: prevent data leakage in feature pipeline

- Move StandardScaler.fit() after train/test split
- Add validation to catch future data leakage
- Update tests to verify proper data isolation

Fixes #456"

# âŒ BAD - Useless commit messages
git commit -m "fix stuff"
git commit -m "updates"
git commit -m "wip"
```

---

## ðŸ“‹ **SIMPLE CHECKLIST**

Before every commit:

- [ ] **Code passes `ruff check`** (linting + formatting)
- [ ] **Type hints on new functions** (`mypy` passes)
- [ ] **Tests for new functionality** (90%+ coverage)
- [ ] **No hardcoded secrets** (use environment variables)
- [ ] **Performance considered** (async for I/O, generators for large data)
- [ ] **Security checked** (input validation, no injection risks)

Before every PR:

- [ ] **Branch is up to date** with main
- [ ] **PR description explains the change**
- [ ] **Breaking changes documented**
- [ ] **Deployment impact considered**

---

## ðŸš« **TOP 4 CRITICAL MISTAKES TO AVOID**

```python
# âŒ 1. DON'T: Hardcode secrets
API_KEY = "sk-1234567890"  # Security nightmare!

# âœ… DO: Use environment variables
api_key = os.getenv("API_KEY")

# âŒ 2. DON'T: Load huge datasets into memory
df = pd.read_csv("100GB_file.csv")  # Memory explosion!

# âœ… DO: Process in chunks
for chunk in pd.read_csv("100GB_file.csv", chunksize=10000):
    process_chunk(chunk)

# âŒ 3. DON'T: Skip input validation
def predict(user_input):
    return model.predict(user_input)  # No validation!

# âœ… DO: Validate all inputs
def predict(user_input: PredictionRequest):
    # Validation happens in Pydantic model
    return model.predict(user_input.features)

# âŒ 4. DON'T: Ignore testing
# No tests = broken in production

# âœ… DO: Test critical functionality
def test_model_accuracy():
    assert model_accuracy > 0.9
```

---

## ðŸŽ¯ **REMEMBER**

> **"Good rules make good code automatic, not burdensome."**

**These rules work because they:**

- **Prevent real problems** (security, performance, bugs)
- **Make debugging easier** (clear structure, good logs)
- **Speed up development** (async, caching, automation)
- **Improve team velocity** (consistent patterns, good tests)

**The goal isn't perfect code - it's reliable ML systems that solve real problems!** ðŸš€