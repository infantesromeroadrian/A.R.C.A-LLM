---
alwaysApply: true
---
### **Stage 2: ü§ñ Features ‚Üí Model (1-3 days)**
```python
# workflows/model_workflow.py
import mlflow
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report

class ModelWorkflow:
    """Simple model development workflow."""
    
    def __init__(self, experiment_name: str = "ml_experiment"):
        self.experiment_name = experiment_name
        mlflow.set_experiment(experiment_name)
    
    def run(self, train_data: pd.DataFrame, test_data: pd.DataFrame) -> dict:
        """Execute complete model workflow."""
        print("üîÑ Stage 2: Features ‚Üí Model")
        
        with mlflow.start_run() as run:
            # 1. Prepare data
            X_train, y_train = self._prepare_features(train_data)
            X_test, y_test = self._prepare_features(test_data)
            
            # 2. Train model
            model = self._train_model(X_train, y_train)
            
            # 3. Evaluate model
            metrics = self._evaluate_model(model, X_test, y_test)
            
            # 4. Log everything
            self._log_experiment(model, metrics, run.info.run_id)
            
            print(f"‚úÖ Model trained: {metrics['accuracy']:.3f} accuracy")
            return {
                'model': model,
                'metrics': metrics,
                'run_id': run.info.run_id
            }
    
    def _prepare_features(self, data: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:
        """Prepare features and target."""
        X = data.drop('target', axis=1)
        y = data['target']
        return X, y
    
    def _train_model(self, X_train: pd.DataFrame, y_train: pd.Series) -> RandomForestClassifier:
        """Train model with good defaults."""
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1  # Use all CPU cores
        )
        model.fit(X_train, y_train)
        return model
    
    def _evaluate_model(self, model, X_test: pd.DataFrame, y_test: pd.Series) -> dict:
        """Evaluate model performance."""
        predictions = model.predict(X_test)
        
        from sklearn.metrics import accuracy_score, precision_recall_fscore_support
        
        accuracy = accuracy_score(y_test, predictions)
        precision, recall, f1, _ = precision_recall_fscore_support(
            y_test, predictions, average='weighted'
        )
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall, 
            'f1_score': f1
        }
    
    def _log_experiment(self, model, metrics: dict, run_id: str):
        """Log experiment to MLflow."""
        # Log metrics
        mlflow.log_metrics(metrics)
        
        # Log model
        mlflow.sklearn.log_model(model, "model")
        
        # Log feature importance
        if hasattr(model, 'feature_importances_'):
            importance_dict = {
                f"feature_importance_{i}": imp 
                for i, imp in enumerate(model.feature_importances_)
            }
            mlflow.log_metrics(importance_dict)
```

---

### **Stage 3: üöÄ Model ‚Üí API (1 day)**
```python
# workflows/deployment_workflow.py
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np

class DeploymentWorkflow:
    """Simple deployment workflow for ML models."""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.app = FastAPI(title="ML Model API")
        self._setup_routes()
    
    def _setup_routes(self):
        """Setup API routes for model."""
        
        @self.app.on_event("startup")
        async def load_model():
            """Load model on startup."""
            self.model = joblib.load(self.model_path)
            print(f"‚úÖ Model loaded from {self.model_path}")
        
        @self.app.get("/health")
        async def health_check():
            """Health check endpoint."""
            return {
                "status": "healthy" if self.model is not None else "unhealthy",
                "model_loaded": self.model is not None
            }
        
        @self.app.post("/predict")
        async def predict(request: PredictionRequest):
            """Make prediction."""
            if self.model is None:
                raise HTTPException(status_code=503, detail="Model not loaded")
            
            try:
                # Convert input to numpy array
                features = np.array(request.features).reshape(1, -1)
                
                # Make prediction
                prediction = self.model.predict(features)[0]
                confidence = max(self.model.predict_proba(features)[0])
                
                return PredictionResponse(
                    prediction=int(prediction),
                    confidence=float(confidence)
                )
                
            except Exception as e:
                raise HTTPException(status_code=400, detail=f"Prediction failed: {e}")
    
    def deploy_locally(self, port: int = 8000):
        """Deploy model locally for testing."""
        import uvicorn
        print(f"üöÄ Stage 3: Model ‚Üí API (localhost:{port})")
        uvicorn.run(self.app, host="0.0.0.0", port=port)

class PredictionRequest(BaseModel):
    """Prediction request schema."""
    features: list[float]

class PredictionResponse(BaseModel):
    """Prediction response schema."""
    prediction: int
    confidence: float

# Usage - Deploy any trained model
deployment = DeploymentWorkflow("trained_model.pkl")
deployment.deploy_locally(port=8000)
```

---

### **Stage 4: üìà API ‚Üí Monitoring (Ongoing)**
```python
# workflows/monitoring_workflow.py
import time
from datetime import datetime, timedelta
from typing import Dict, List
import requests

class MonitoringWorkflow:
    """Simple monitoring for deployed ML models."""
    
    def __init__(self, api_url: str):
        self.api_url = api_url
        self.metrics_history: List[Dict] = []
    
    def check_model_health(self) -> Dict[str, any]:
        """Check if model API is healthy."""
        print("üîÑ Stage 4: API ‚Üí Monitoring")
        
        try:
            # Health check
            response = requests.get(f"{self.api_url}/health", timeout=5)
            health_status = response.json()
            
            # Performance check
            performance = self._check_performance()
            
            # Data drift check (simplified)
            drift_score = self._check_data_drift()
            
            current_health = {
                'timestamp': datetime.now().isoformat(),
                'api_healthy': response.status_code == 200,
                'model_loaded': health_status.get('model_loaded', False),
                'avg_response_time': performance['avg_response_time'],
                'error_rate': performance['error_rate'],
                'data_drift_score': drift_score
            }
            
            self.metrics_history.append(current_health)
            
            # Alert if issues detected
            alerts = self._check_alerts(current_health)
            
            return {
                'status': 'healthy' if not alerts else 'degraded',
                'current_metrics': current_health,
                'alerts': alerts,
                'trend': self._calculate_trend()
            }
            
        except requests.RequestException as e:
            return {
                'status': 'unhealthy',
                'error': f"API unreachable: {e}",
                'alerts': [{'type': 'api_down', 'severity': 'critical'}]
            }
    
    def _check_performance(self) -> Dict[str, float]:
        """Check API performance."""
        test_requests = 10
        response_times = []
        errors = 0
        
        for _ in range(test_requests):
            try:
                start_time = time.time()
                response = requests.post(
                    f"{self.api_url}/predict",
                    json={"features": [1.0, 2.0, 3.0, 4.0, 5.0]},
                    timeout=2
                )
                response_time = time.time() - start_time
                response_times.append(response_time)
                
                if response.status_code != 200:
                    errors += 1
                    
            except requests.RequestException:
                errors += 1
                response_times.append(2.0)  # Timeout value
        
        return {
            'avg_response_time': np.mean(response_times),
            'error_rate': errors / test_requests
        }
    
    def _check_data_drift(self) -> float:
        """Simplified data drift detection."""
        # In production, this would compare current inputs with training data
        # For now, return a mock drift score
        return np.random.uniform(0, 0.3)  # 0-30% drift
    
    def _check_alerts(self, current_metrics: Dict) -> List[Dict]:
        """Check for alerting conditions."""
        alerts = []
        
        # Performance alerts
        if current_metrics['avg_response_time'] > 1.0:  # 1 second threshold
            alerts.append({
                'type': 'slow_response',
                'severity': 'medium',
                'message': f"Response time {current_metrics['avg_response_time']:.2f}s > 1.0s"
            })
        
        if current_metrics['error_rate'] > 0.05:  # 5% error rate
            alerts.append({
                'type': 'high_error_rate', 
                'severity': 'high',
                'message': f"Error rate {current_metrics['error_rate']:.1%} > 5%"
            })
        
        # Data drift alerts
        if current_metrics['data_drift_score'] > 0.3:  # 30% drift
            alerts.append({
                'type': 'data_drift',
                'severity': 'medium', 
                'message': f"Data drift {current_metrics['data_drift_score']:.1%} > 30%"
            })
        
        return alerts
    
    def _calculate_trend(self) -> str:
        """Calculate performance trend."""
        if len(self.metrics_history) < 2:
            return "insufficient_data"
        
        recent_perf = self.metrics_history[-1]['avg_response_time']
        previous_perf = self.metrics_history[-2]['avg_response_time']
        
        if recent_perf < previous_perf * 0.9:
            return "improving"
        elif recent_perf > previous_perf * 1.1:
            return "degrading"
        else:
            return "stable"

# Usage - Simple monitoring
monitor = MonitoringWorkflow("http://localhost:8000")
health = monitor.check_model_health()
print(f"Model status: {health['status']}")
```

---

## üîÅ **COMPLETE WORKFLOW EXAMPLE**

Put it all together in one simple script:

```python
# run_ml_workflow.py
import asyncio
from pathlib import Path

async def run_complete_ml_workflow(data_path: Path, model_name: str):
    """Run complete ML workflow from data to deployed model."""
    
    try:
        # Stage 1: Data ‚Üí Features
        data_workflow = DataWorkflow(data_path)
        train_data, test_data = data_workflow.run()
        
        # Stage 2: Features ‚Üí Model  
        model_workflow = ModelWorkflow(experiment_name=model_name)
        result = model_workflow.run(train_data, test_data)
        
        # Save trained model
        model_path = f"models/{model_name}.pkl"
        joblib.dump(result['model'], model_path)
        
        # Stage 3: Model ‚Üí API (background deployment)
        deployment = DeploymentWorkflow(model_path)
        print("üöÄ Starting API server...")
        
        # Run deployment in background
        import subprocess
        subprocess.Popen([
            "python", "-m", "uvicorn", 
            "workflows.deployment_workflow:deployment.app",
            "--host", "0.0.0.0", "--port", "8000"
        ])
        
        # Wait for API to start
        await asyncio.sleep(5)
        
        # Stage 4: API ‚Üí Monitoring
        monitor = MonitoringWorkflow("http://localhost:8000")
        health = monitor.check_model_health()
        
        print("üèÜ Workflow Complete!")
        print(f"üìä Model Metrics: {result['metrics']}")
        print(f"üîç API Health: {health['status']}")
        print(f"üåê API URL: http://localhost:8000")
        
        return {
            'status': 'success',
            'model_metrics': result['metrics'],
            'api_health': health,
            'endpoints': {
                'health': 'http://localhost:8000/health',
                'predict': 'http://localhost:8000/predict'
            }
        }
        
    except Exception as e:
        print(f"‚ùå Workflow failed: {e}")
        return {'status': 'failed', 'error': str(e)}

# Usage - Complete ML workflow in one command
if __name__ == "__main__":
    import sys
    
    if len(sys.argv) != 3:
        print("Usage: python run_ml_workflow.py <data_path> <model_name>")
        sys.exit(1)
    
    data_path = Path(sys.argv[1])
    model_name = sys.argv[2]
    
    result = asyncio.run(run_complete_ml_workflow(data_path, model_name))
    
    if result['status'] == 'success':
        print("\nüéâ SUCCESS! Your ML model is live!")
        print(f"   Test it: curl -X POST http://localhost:8000/predict -d '{{\"features\": [1,2,3,4,5]}}'")
    else:
        print(f"\nüí• FAILED: {result['error']}")
```

---

## üìã **DAILY WORKFLOW CHECKLIST**

### **Starting a New Model (5 minutes):**
```markdown
‚ñ° Create git branch: `feature/model-{name}`
‚ñ° Add data to `data/` directory
‚ñ° Run data workflow: `python workflows/data_workflow.py`
‚ñ° Check data quality passes (>80% completion)
‚ñ° Commit initial data analysis
```

### **Developing Model (1-3 days):**
```markdown
‚ñ° Run model workflow: `python workflows/model_workflow.py`
‚ñ° Check performance meets target (>85% accuracy)
‚ñ° Log experiment to MLflow
‚ñ° Save trained model artifact
‚ñ° Commit model code and metrics
```

### **Deploying Model (30 minutes):**
```markdown
‚ñ° Test model locally with deployment workflow
‚ñ° Check API health endpoint responds
‚ñ° Test prediction endpoint with sample data
‚ñ° Deploy to staging environment (if available)
‚ñ° Run basic smoke tests
```

### **Monitoring Model (Daily 2 minutes):**
```markdown
‚ñ° Check model health dashboard
‚ñ° Review performance metrics vs baseline
‚ñ° Check for alerts or degradation
‚ñ° Update model if significant drift detected
```

---

## ‚ö° **AUTOMATED CI/CD (SIMPLE VERSION)**

```yaml
# .github/workflows/ml-pipeline.yml
name: Simple ML Pipeline

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  test-workflow:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          
      - name: Test data workflow
        run: |
          python -m pytest tests/test_data_workflow.py -v
          
      - name: Test model workflow  
        run: |
          python -m pytest tests/test_model_workflow.py -v
          
      - name: Check model performance
        run: |
          python scripts/validate_model_performance.py
          
  deploy:
    needs: test-workflow
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    
    steps:
      - uses: actions/checkout@v4
      
      - name: Build Docker image
        run: |
          docker build -t ml-model:latest .
          
      - name: Deploy to staging
        run: |
          echo "üöÄ Deploying to staging..."
          # Add your deployment commands here
```

---

## üìä **SIMPLE SUCCESS METRICS**

Track what matters for ML workflows:

```python
@dataclass
class WorkflowMetrics:
    """Track simple workflow success metrics."""
    
    # Speed metrics
    data_to_model_hours: float = 0
    model_to_api_hours: float = 0
    total_workflow_hours: float = 0
    
    # Quality metrics
    model_accuracy: float = 0
    api_uptime_percent: float = 0
    error_rate_percent: float = 0
    
    # Business metrics
    deployments_this_week: int = 0
    models_in_production: int = 0
    
    def calculate_workflow_grade(self) -> str:
        """Simple A-F grade for workflow health."""
        score = 0
        
        # Speed (25%)
        if self.total_workflow_hours <= 24: score += 25
        elif self.total_workflow_hours <= 72: score += 15
        elif self.total_workflow_hours <= 168: score += 10
        
        # Quality (50%)
        if self.model_accuracy >= 0.9: score += 25
        elif self.model_accuracy >= 0.85: score += 20
        elif self.model_accuracy >= 0.8: score += 15
        
        if self.api_uptime_percent >= 99: score += 25
        elif self.api_uptime_percent >= 95: score += 20
        elif self.api_uptime_percent >= 90: score += 15
        
        # Productivity (25%)
        if self.deployments_this_week >= 3: score += 25
        elif self.deployments_this_week >= 2: score += 20
        elif self.deployments_this_week >= 1: score += 15
        
        if score >= 90: return "A"
        if score >= 80: return "B" 
        if score >= 70: return "C"
        if score >= 60: return "D"
        return "F"

# Usage - Track your workflow performance
metrics = WorkflowMetrics(
    data_to_model_hours=4,
    model_to_api_hours=1,
    total_workflow_hours=5,
    model_accuracy=0.92,
    api_uptime_percent=99.5,
    deployments_this_week=2
)

print(f"Workflow Grade: {metrics.calculate_workflow_grade()}")  # Grade: A
```

---

## üéØ **IMPLEMENTATION PLAN**

### **Week 1: Basic Workflow**
```markdown
‚ñ° Set up the 4 workflow stages
‚ñ° Test with simple dataset
‚ñ° Deploy first model locally
‚ñ° Basic monitoring in place
```

### **Week 2: Automation**
```markdown
‚ñ° Add GitHub Actions CI/CD
‚ñ° Set up MLflow experiment tracking
‚ñ° Create Docker deployment
‚ñ° Add basic alerting
```

### **Week 3: Production Ready**
```markdown
‚ñ° Deploy to cloud environment
‚ñ° Set up proper monitoring dashboards
‚ñ° Add data drift detection
‚ñ° Document runbooks
```

---

## üí° **REMEMBER**

> **"Workflows should make hard things easy, not easy things hard."**

**Good ML workflows:**
- **Start simple** and add complexity only when needed
- **Automate the boring parts** (testing, deployment, monitoring)
- **Make failures visible** and recoverable quickly
- **Focus on business value** not engineering perfection
- **Evolve with your team** and requirements

**The goal isn't the perfect workflow - it's shipping models that solve real problems consistently!** üöÄ
