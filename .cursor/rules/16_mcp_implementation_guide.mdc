---
description: Complete guide for implementing MCP (Model Context Protocol) projects with FastAPI, LLM integration, and modern frontend
alwaysApply: false
---
# üîó MCP Implementation Guide - From Zero to Hero

## üéØ **Philosophy: Smart Tools + LLM Intelligence = Magic**

> *"Don't hardcode what the LLM can figure out naturally. Create tools, provide context, let intelligence work."*

---

## üèóÔ∏è **THE MCP PROJECT STRUCTURE**

### **Essential Directory Layout**

```text
üìÅ mcp-project/
‚îú‚îÄ‚îÄ üìÅ src/
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ api/                 # FastAPI presentation layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py            # FastAPI app with MCP integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Pydantic models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ chat_routes.py # Chat endpoints
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ chat/               # Business logic layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py         # Conversational AI service
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ frontend/           # Modern UI layer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ css/              # Modular CSS
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ js/               # Modular JavaScript
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ html/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ index.html    # Single-page interface
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ llm/               # Infrastructure layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ client.py         # LLM communication
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ mcp_client/        # Data layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ connector.py      # MCP protocol handler
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ server/            # MCP server
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ server-demo1.py   # MCP tools definition
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ tools/             # Tool implementations
‚îÇ       ‚îú‚îÄ‚îÄ math_operations.py
‚îÇ       ‚îú‚îÄ‚îÄ database_operations.py
‚îÇ       ‚îú‚îÄ‚îÄ weather_operations.py
‚îÇ       ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ üìÅ data/                   # SQLite databases
‚îú‚îÄ‚îÄ üìÑ claude_desktop_config.json  # Claude integration
‚îú‚îÄ‚îÄ üìÑ requirements.txt        # Dependencies
‚îî‚îÄ‚îÄ üìÑ .env                    # Environment variables
```

---

## üöÄ **STEP 1: MCP SERVER FOUNDATION**

### **Create FastMCP Server** (src/server/server-demo1.py)

```python
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from mcp.server.fastmcp import FastMCP
from tools import *  # Import all tool implementations

# Create MCP server
mcp = FastMCP("your-project-name")

# ============ MATH TOOLS ============
@mcp.tool()
def add(a: int, b: int) -> int:
    """Add two numbers"""
    return add_numbers(a, b)

@mcp.tool()
def multiply(a: int, b: int) -> int:
    """Multiply two numbers"""
    return multiply_numbers(a, b)

# ============ DATABASE TOOLS ============
@mcp.tool()
def list_all_users() -> str:
    """Get all users from the database"""
    return get_all_users()

@mcp.tool()
def find_user_by_id(user_id: int) -> str:
    """Find a specific user by their ID"""
    return get_user_by_id(user_id)

# ============ WEATHER TOOLS ============
@mcp.tool()
def get_weather(query: str) -> str:
    """Get current weather for any city worldwide from natural language"""
    return get_weather_by_city(query)

# ============ MCP RESOURCES ============
@mcp.resource("users://all")
def users_resource() -> str:
    """All users database as LLM context"""
    return f"# All Users\n{get_all_users()}"

@mcp.resource("products://all") 
def products_resource() -> str:
    """All products catalog as LLM context"""
    return f"# All Products\n{get_all_products()}"

# Run server
if __name__ == "__main__":
    mcp.run(transport="stdio")  # Claude Desktop compatibility
```

**Key Patterns:**
- ‚úÖ **@mcp.tool()** for executable functions
- ‚úÖ **@mcp.resource()** for LLM context
- ‚úÖ **Descriptive docstrings** - LLM reads these
- ‚úÖ **Transport="stdio"** for Claude Desktop

---

## üîß **STEP 2: TOOL IMPLEMENTATIONS**

### **Organize Tools by Domain** (src/tools/)

```python
# src/tools/math_operations.py
def add_numbers(a: int, b: int) -> int:
    """Simple, pure function implementation"""
    return a + b

def power_numbers(base: float, exponent: float) -> float:
    """Calculate base raised to power"""
    return base ** exponent

# src/tools/database_operations.py
import sqlite3
import json

def get_all_users() -> str:
    """Get all users - Return JSON string for LLM parsing"""
    try:
        with get_db() as conn:
            users = [dict(row) for row in conn.execute("SELECT * FROM users")]
            return f"Found {len(users)} users:\n{json.dumps(users, indent=2)}"
    except Exception as e:
        return f"Error: {e}"

# src/tools/weather_operations.py
import aiohttp
import asyncio

def get_weather_by_city(city_query: str) -> str:
    """Get weather - Handle natural language queries"""
    try:
        city = extract_city(city_query)
        # Async weather API call with proper formatting
        return run_async(fetch_and_format_weather(city))
    except Exception as e:
        return f"‚ùå Error: {str(e)}"
```

**Tool Implementation Rules:**
- ‚úÖ **Pure functions** - No side effects where possible
- ‚úÖ **String returns** - LLM can parse any format
- ‚úÖ **Error handling** - Always return string, never throw
- ‚úÖ **JSON formatting** - For structured data
- ‚úÖ **Natural language input** - Let LLM handle extraction

---

## üåê **STEP 3: FASTAPI + MCP INTEGRATION**

### **Main FastAPI App** (src/api/main.py)

```python
from contextlib import asynccontextmanager
from fastapi import FastAPI
from mcp_client.connector import MCPConnector

# Global MCP connector
mcp_connector = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage MCP connection lifecycle"""
    global mcp_connector
    
    # Startup - Connect to MCP
    print("üîó Connecting to MCP server...")
    mcp_connector = MCPConnector()
    
    connected = await mcp_connector.connect("python", ["src/server/server-demo1.py"])
    if not connected:
        raise RuntimeError("MCP server connection failed")
    
    print(f"‚úÖ MCP connected | {len(mcp_connector.get_tools())} tools available")
    
    yield  # Application running
    
    # Shutdown
    await mcp_connector.cleanup()

# Create FastAPI with lifespan
app = FastAPI(
    title="MCP Assistant API",
    version="1.0.0",
    lifespan=lifespan
)

# Health check
@app.get("/health")
async def health_check():
    tools_count = len(mcp_connector.get_tools()) if mcp_connector else 0
    return {"status": "healthy", "available_tools": tools_count}

# MCP tools endpoint
@app.get("/api/tools")
async def list_tools():
    """List all available MCP tools with categories"""
    tools = mcp_connector.get_tools()
    
    # Auto-categorize tools
    categorized_tools = []
    for tool in tools:
        name = tool.get('name', '')
        category = categorize_tool(name)  # Smart categorization
        
        categorized_tools.append({
            "name": name,
            "description": tool.get('description', ''),
            "category": category,
            "parameters": tool.get('schema', {})
        })
    
    return {"success": True, "tools": categorized_tools}

# MCP resources endpoint
@app.get("/api/resources")
async def list_resources():
    """List all available MCP resources"""
    resources = await mcp_connector.list_resources()
    return {"success": True, "resources": resources}
```

**Integration Patterns:**
- ‚úÖ **Lifespan management** for MCP connection
- ‚úÖ **Global connector** accessible to all routes
- ‚úÖ **Auto-categorization** of tools
- ‚úÖ **Proper async/await** throughout

---

## üß† **STEP 4: INTELLIGENT CHAT SERVICE**

### **Chat Service with Memory** (src/chat/service.py)

```python
class ChatService:
    """Natural LLM chat with MCP tools and conversational memory"""
    
    def __init__(self, llm_client: LLMClient, mcp_connector: MCPConnector):
        self.llm_client = llm_client
        self.mcp_connector = mcp_connector
        self._last_tool_used = None
        
        # üß† CONVERSATIONAL MEMORY
        self.conversation_history = []
        self.max_history_length = 10
    
    async def process_message(self, user_message: str) -> str:
        """Process message with LLM intelligence + memory"""
        self._last_tool_used = None
        
        # Add to conversation history
        self._add_to_history("user", user_message)
        
        # Get available tools for LLM context
        tools_context = self._build_tools_context(self.mcp_connector.get_tools())
        
        # Let LLM decide intelligently what to do
        response = await self._handle_intelligent_request(user_message, tools_context)
        
        # Add response to history
        self._add_to_history("assistant", response, self._last_tool_used)
        
        return response
    
    async def _handle_intelligent_request(self, user_message: str, tools_context: str) -> str:
        """Handle request with pure LLM intelligence"""
        
        # Build conversation context
        conversation_context = self._build_conversation_context()
        
        # Language detection
        is_spanish = any(word in user_message.lower() for word in 
                        ['cu√°l', 'cu√°ntos', 'clima', 'tiempo', 'usuarios'])
        
        # LLM analysis prompt
        prompt = f"""You are an intelligent assistant. {conversation_context}

Available tools:
{tools_context}

User asks: "{user_message}"

Decide if you need to use any tool. If yes, respond: "USE_TOOL: tool_name {{arguments}}"
If you have info from previous conversation, respond directly.

Examples:
- "how many users?" ‚Üí USE_TOOL: list_all_users {{}}
- "weather in Madrid" ‚Üí USE_TOOL: get_weather {{"query": "Madrid"}}
- "what's the most expensive?" (after showing products) ‚Üí Based on previous data...
"""
        
        llm_response = await self.llm_client.generate_response(prompt)
        
        # Execute tool if LLM decided to use one
        if "USE_TOOL:" in llm_response:
            return await self._execute_llm_tool_request(llm_response, user_message)
        
        return llm_response.strip()
```

**Chat Service Patterns:**
- ‚úÖ **LLM decides tools** - No hardcoded logic
- ‚úÖ **Conversational memory** - Context awareness
- ‚úÖ **Language detection** - Bilingual support
- ‚úÖ **Smart tool parsing** - Flexible argument handling

---

## üíª **STEP 5: MODERN FRONTEND INTEGRATION**

### **Frontend Architecture** (src/frontend/)

```javascript
// src/frontend/js/main.js - Application coordinator
const AppState = {
    isInitialized: false,
    currentView: 'chat',
    connectionStatus: 'disconnected'
};

async function initializeApp() {
    // Check module dependencies
    if (!window.Utils || !window.API || !window.Chat || !window.Tools) {
        throw new Error('Required modules not loaded');
    }
    
    // Initialize components
    initializeNavigation();
    Chat.initializeChat();
    await Tools.initializeTools();
    await initializeConnectionMonitoring();
    
    AppState.isInitialized = true;
    console.log('‚úÖ MCP Application initialized');
}

// src/frontend/js/chat.js - Chat interface
async function processUserMessage(message) {
    try {
        // Direct integration with MCP chat service
        const response = await fetch('/api/chat/message', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ message })
        });
        
        const data = await response.json();
        return data.response;  // LLM + MCP response
        
    } catch (error) {
        return `‚ùå Error: ${error.message}`;
    }
}

// src/frontend/js/api.js - Simple HTTP client
const API = {
    checkHealth: () => get('/health'),
    getAvailableTools: () => get('/api/tools'),
    getAvailableResources: () => get('/api/resources'),
    sendChatMessage: (message) => post('/api/chat/message', { message })
};
```

**Frontend Patterns:**
- ‚úÖ **Modular JavaScript** - Separate concerns
- ‚úÖ **Direct MCP integration** - Chat calls MCP backend
- ‚úÖ **Multi-view interface** - Chat, Tools, Resources, Database
- ‚úÖ **Responsive design** - Mobile-first CSS

---

## üîå **STEP 6: LLM CLIENT INTEGRATION**

### **LLM Client** (src/llm/client.py)

```python
class LLMClient:
    """Handles communication with local LLM (LM Studio/Ollama)"""
    
    def __init__(self, llm_url: str, model_name: str):
        self.llm_url = llm_url  # e.g., "http://10.2.0.2:1234/v1"
        self.model_name = model_name  # e.g., "qwen/qwen3-4b-2507:2"
    
    async def generate_response(self, prompt: str, max_tokens: int = 300) -> str:
        """Generate response using local LLM"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                payload = {
                    "model": self.model_name,
                    "messages": [
                        {"role": "system", "content": "You are a helpful assistant."},
                        {"role": "user", "content": prompt}
                    ],
                    "max_tokens": max_tokens,
                    "temperature": 0.3
                }
                
                async with session.post(f"{self.llm_url}/chat/completions", json=payload) as response:
                    result = await response.json()
                    return result["choices"][0]["message"]["content"].strip()
                        
        except Exception as e:
            return f"Error: {str(e)}"
```

**LLM Integration Patterns:**
- ‚úÖ **Local LLM support** - LM Studio, Ollama compatible
- ‚úÖ **Async/await** - Non-blocking operations
- ‚úÖ **Timeout handling** - 30 second limit
- ‚úÖ **Error recovery** - Graceful fallbacks

---

## üóÑÔ∏è **STEP 7: DATABASE OPERATIONS**

### **SQLite Integration** (src/tools/database_operations.py)

```python
import sqlite3
import json
from pathlib import Path

DB_PATH = "data/mcp_database.db"

def get_db():
    """Get database connection with auto-initialization"""
    Path("data").mkdir(exist_ok=True)
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    
    # Create tables if they don't exist
    conn.execute("""CREATE TABLE IF NOT EXISTS users (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        name TEXT NOT NULL,
        email TEXT UNIQUE NOT NULL,
        age INTEGER,
        city TEXT,
        created_at DATETIME DEFAULT CURRENT_TIMESTAMP
    )""")
    
    # Insert sample data if empty
    if conn.execute("SELECT COUNT(*) FROM users").fetchone()[0] == 0:
        sample_users = [
            ("Alice Johnson", "alice@email.com", 28, "Madrid"),
            ("Bob Smith", "bob@email.com", 35, "Barcelona"),
            ("Carol Davis", "carol@email.com", 22, "Valencia")
        ]
        conn.executemany("INSERT INTO users (name, email, age, city) VALUES (?, ?, ?, ?)", 
                        sample_users)
        conn.commit()
    
    return conn

def get_all_users() -> str:
    """Get all users - MCP tool implementation"""
    try:
        with get_db() as conn:
            users = [dict(row) for row in conn.execute("SELECT * FROM users ORDER BY name")]
            return f"Found {len(users)} users:\n{json.dumps(users, indent=2)}"
    except Exception as e:
        return f"Error: {e}"
```

**Database Patterns:**
- ‚úÖ **SQLite for simplicity** - No complex setup
- ‚úÖ **Auto-initialization** - Creates tables and sample data
- ‚úÖ **JSON string returns** - LLM can parse naturally
- ‚úÖ **Context managers** - Automatic connection cleanup

---

## üåç **STEP 8: EXTERNAL API INTEGRATION**

### **Weather API Integration** (src/tools/weather_operations.py)

```python
import aiohttp
import asyncio
import os

class WeatherAPI:
    """OpenWeatherMap API wrapper"""
    
    def __init__(self):
        self.api_key = os.getenv('OPENWEATHERMAP_API_KEY', 'your-default-key')
        self.weather_url = "https://api.openweathermap.org/data/2.5/weather"
        self.geocoding_url = "http://api.openweathermap.org/geo/1.0/direct"
    
    async def get_coordinates(self, city: str) -> Optional[tuple]:
        """Get coordinates for any city worldwide"""
        try:
            params = {'q': city, 'limit': 1, 'appid': self.api_key}
            async with aiohttp.ClientSession() as session:
                async with session.get(self.geocoding_url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        if data:
                            return (data[0]['lat'], data[0]['lon'])
            return None
        except Exception:
            return None
    
    def format_weather(self, data: Dict[str, Any], city: str) -> str:
        """Format weather data for LLM consumption"""
        main = data.get('main', {})
        weather = data.get('weather', [{}])[0]
        
        return f"""üå§Ô∏è **Weather in {city}**

üå°Ô∏è **Temperature**: {main.get('temp', 0)}¬∞C (feels like: {main.get('feels_like', 0)}¬∞C)
‚òÅÔ∏è **Conditions**: {weather.get('description', 'N/A').title()}
üíß **Humidity**: {main.get('humidity', 0)}%
üå™Ô∏è **Wind**: {data.get('wind', {}).get('speed', 0)} m/s"""

def get_weather_by_city(city_query: str) -> str:
    """MCP tool implementation for weather"""
    try:
        city = extract_city(city_query)  # Simple extraction
        return run_async(fetch_weather_async(city))
    except Exception as e:
        return f"‚ùå Error: {str(e)}"
```

**External API Patterns:**
- ‚úÖ **Async API calls** - Non-blocking operations
- ‚úÖ **Environment variables** - API keys from .env
- ‚úÖ **Natural language processing** - City extraction
- ‚úÖ **Formatted responses** - Rich text for LLM

---

## ü§ñ **STEP 9: MCP CLIENT CONNECTOR**

### **MCP Protocol Handler** (src/mcp_client/connector.py)

```python
from mcp.client.session import ClientSession
from mcp.client.stdio import StdioServerParameters, stdio_client
from contextlib import AsyncExitStack

class MCPConnector:
    """Handles MCP server connection and tool management"""
    
    def __init__(self):
        self.session: Optional[ClientSession] = None
        self.tools: List[Dict] = []
        self.exit_stack = AsyncExitStack()
    
    async def connect(self, server_command: str, server_args: List[str]) -> bool:
        """Connect to MCP server and discover tools"""
        try:
            server_params = StdioServerParameters(
                command=server_command,
                args=server_args,
                cwd=os.getcwd()
            )
            
            # Establish stdio transport
            stdio_transport = await self.exit_stack.enter_async_context(
                stdio_client(server_params)
            )
            read_stream, write_stream = stdio_transport
            
            # Create session
            self.session = await self.exit_stack.enter_async_context(
                ClientSession(read_stream, write_stream)
            )
            
            await self.session.initialize()
            
            # Discover tools automatically
            tools_response = await self.session.list_tools()
            self.tools = [
                {
                    "name": tool.name,
                    "description": tool.description,
                    "schema": tool.inputSchema
                }
                for tool in tools_response.tools
            ]
            
            return True
            
        except Exception as e:
            print(f"‚ùå MCP connection failed: {e}")
            return False
    
    async def execute_tool(self, tool_name: str, args: Dict[str, Any]) -> str:
        """Execute MCP tool with arguments"""
        try:
            result = await self.session.call_tool(tool_name, arguments=args)
            return result.content[0].text if result.content else "No result"
        except Exception as e:
            return f"Error executing {tool_name}: {e}"
```

**MCP Client Patterns:**
- ‚úÖ **AsyncExitStack** - Proper resource management
- ‚úÖ **Auto-discovery** - Tools loaded dynamically
- ‚úÖ **Clean abstraction** - Hide MCP complexity
- ‚úÖ **Error isolation** - Tool failures don't crash system

---

## ‚öôÔ∏è **STEP 10: CONFIGURATION & DEPLOYMENT**

### **Claude Desktop Integration** (claude_desktop_config.json)

```json
{
    "mcpServers": {
        "your-project-name": {
            "command": "python",
            "args": ["src/server/server-demo1.py"],
            "cwd": "/path/to/your/project",
            "env": {
                "PYTHONPATH": "src"
            }
        }
    }
}
```

### **Environment Configuration** (.env)

```bash
# LLM Configuration
LLM_URL=http://10.2.0.2:1234/v1
LLM_MODEL=qwen/qwen3-4b-2507:2

# API Keys
OPENWEATHERMAP_API_KEY=your_api_key_here

# Database
DATABASE_PATH=data/mcp_database.db

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
CORS_ORIGINS=["http://localhost:8000"]
```

### **Dependencies** (requirements.txt)

```txt
# MCP Framework
mcp>=1.0.0
fastmcp>=0.3.0

# FastAPI Stack
fastapi>=0.104.0
uvicorn>=0.24.0
pydantic>=2.5.0

# LLM Integration
aiohttp>=3.9.0

# Database
sqlite3  # Built-in with Python

# Environment
python-dotenv>=1.0.0

# External APIs (optional)
requests>=2.31.0
```

---

## üöÄ **QUICK START IMPLEMENTATION**

### **30-Minute MCP Setup**

```bash
# 1. Create project structure
mkdir my-mcp-project && cd my-mcp-project
mkdir -p src/{api,chat,frontend/{css,js,html},llm,mcp_client,server,tools} data

# 2. Install dependencies
pip install mcp fastmcp fastapi uvicorn aiohttp python-dotenv

# 3. Create MCP server (src/server/server.py)
# Copy the FastMCP server template above

# 4. Create tool implementations (src/tools/)
# Copy the tool implementation patterns above

# 5. Create FastAPI integration (src/api/main.py)
# Copy the FastAPI + MCP integration template above

# 6. Create chat service (src/chat/service.py)
# Copy the intelligent chat service template above

# 7. Create frontend (src/frontend/)
# Copy the modern frontend templates above

# 8. Configure Claude Desktop
# Update claude_desktop_config.json with your project path

# 9. Run the system
python -m uvicorn src.api.main:app --reload --port 8000
```

---

## üìã **MCP PROJECT CHECKLIST**

### **Core MCP Implementation:**
- [ ] **FastMCP server** with @mcp.tool() decorators
- [ ] **Tool implementations** in separate modules  
- [ ] **MCP resources** for LLM context
- [ ] **Claude Desktop config** for integration

### **API Layer:**
- [ ] **FastAPI application** with MCP connector
- [ ] **Lifespan management** for MCP connection
- [ ] **Health endpoints** for monitoring
- [ ] **CORS configuration** for frontend

### **Intelligence Layer:**
- [ ] **LLM client** for local model communication
- [ ] **Chat service** with conversational memory
- [ ] **Natural language processing** - Let LLM decide tools
- [ ] **Bilingual support** if needed

### **Frontend Layer:**
- [ ] **Multi-view interface** - Chat, Tools, Resources
- [ ] **Modern CSS** with variables and responsive design
- [ ] **Modular JavaScript** with proper separation
- [ ] **Real-time chat** interface with typing indicators

### **Data Layer:**
- [ ] **Database operations** with auto-initialization
- [ ] **External APIs** integration (weather, etc.)
- [ ] **JSON formatting** for LLM consumption
- [ ] **Error handling** that returns strings

---

## üéØ **MCP SUCCESS PATTERNS**

### **‚úÖ DO - Essential Patterns:**

```python
# 1. Let LLM decide tools naturally
@mcp.tool()
def get_weather(query: str) -> str:
    """Get weather for any city from natural language"""
    # LLM extracts city, handles language, formats request
    
# 2. Provide rich context via resources
@mcp.resource("users://all")
def users_context() -> str:
    """All users as LLM context"""
    return format_for_llm(get_all_users())

# 3. Return formatted strings for LLM parsing
def get_user_stats() -> str:
    stats = calculate_user_statistics()
    return f"üìä **User Statistics**\n\n{format_stats(stats)}"

# 4. Use conversational memory
class ChatService:
    def __init__(self):
        self.conversation_history = []
    
    def _build_conversation_context(self) -> str:
        # Provide recent conversation to LLM for context
```

### **‚ùå DON'T - Anti-Patterns:**

```python
# 1. Don't hardcode query patterns
if "weather" in query and "madrid" in query:  # ‚ùå Rigid
    return get_madrid_weather()

# 2. Don't return complex objects from tools
@mcp.tool()
def bad_tool() -> Dict[str, Any]:  # ‚ùå LLM can't use this
    return {"complex": "object"}

# 3. Don't make stateful tools
@mcp.tool()  
def bad_stateful_tool():  # ‚ùå Tools should be pure
    self.internal_state += 1
    return self.internal_state

# 4. Don't skip error handling
@mcp.tool()
def bad_tool():
    return database.query()  # ‚ùå Could throw exceptions
```

---

## üèÜ **QUALITY GATES FOR MCP PROJECTS**

### **Before Deployment:**
- [ ] **All tools return strings** - LLM can parse any response
- [ ] **No hardcoded patterns** - LLM handles natural language
- [ ] **Error handling complete** - Tools never throw exceptions
- [ ] **Resources provide context** - Rich background information
- [ ] **Frontend works without JavaScript** - Progressive enhancement
- [ ] **Local LLM integration** - Works offline if needed
- [ ] **Database auto-initializes** - No manual setup required

### **Performance Checks:**
- [ ] **Tool execution < 5 seconds** - Reasonable response times
- [ ] **Memory usage reasonable** - No memory leaks
- [ ] **Async patterns used** - Non-blocking operations
- [ ] **Connection pooling** - Efficient resource usage

### **Integration Tests:**
- [ ] **Claude Desktop connection** - Works with official client
- [ ] **Natural language queries** - Various phrasings work
- [ ] **Tool combinations** - LLM can chain tools
- [ ] **Error recovery** - Graceful failure handling

---

## üí° **REMEMBER - MCP PHILOSOPHY**

> **"Build tools, not logic. Let intelligence emerge from the combination of simple, well-described capabilities."**

### **MCP Mindset:**
- üß† **Trust LLM intelligence** - Don't over-engineer
- üîß **Create focused tools** - One responsibility each
- üìö **Provide rich context** - Resources feed LLM knowledge
- üåç **Think globally** - Tools work for any input
- ‚ö° **Ship fast** - MCP enables rapid iteration

### **Success Metrics:**
- **Development Speed**: Features in minutes, not days
- **Flexibility**: Handles unexpected queries naturally
- **Maintainability**: Add tools, not logic
- **User Experience**: Natural conversation, no learning curve

---

## üöÄ **THE MCP ADVANTAGE**

**Traditional Development:**
```text
New Feature = API Design + Backend + Frontend + Documentation + Tests
Result: 1-2 weeks per feature
```

**MCP Development:**
```python
@mcp.tool()
def new_feature(query: str) -> str:
    """Description of what this does"""
    return implement_feature(query)

# Result: 30 seconds per feature
```

**Build tools. Trust intelligence. Ship magic.** ‚ú®
