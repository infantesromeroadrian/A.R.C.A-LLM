---
description: 
globs: 
alwaysApply: true
---
---
description: Comprehensive code review guidelines for AI/ML projects with automated analysis, security scanning, and architectural validation.
globs: src/**/*.py, tests/**/*.py, notebooks/**/*.py, scripts/**/*.py, docs/**/*.md
alwaysApply: true
---

# üîç Modern Code Review Guide for AI/ML Projects

## üéØ **Philosophy: Fast, Focused, and Effective Reviews**

> *"The best code review catches bugs before they reach production and teaches the team better practices."*

---

## üöÄ **THE 4-STEP REVIEW PROCESS**

### **Step 1: ‚ö° Automated Checks (2 minutes)**

Before any human touches the code, everything must pass:

```bash
# Modern toolchain - all must pass ‚úÖ
ruff check src/ tests/           # Super fast linting
black --check src/ tests/        # Formatting
mypy src/                        # Type checking
pytest tests/ --cov=src --cov-fail-under=90  # Tests + coverage
bandit -r src/ -q               # Security scan

# AI/ML specific checks
python scripts/check_data_leakage.py    # Prevent train/test contamination
python scripts/validate_model_interfaces.py  # Check model contracts
```

**If any automated check fails ‚Üí NO HUMAN REVIEW**

---

### **Step 2: üéØ Fast Quality Check (5 minutes)**

Quick scan for the **Big 4 Issues**:

#### **üö® 1. Data Leakage (Auto-reject if found)**

```python
# ‚ùå REJECT - Data leakage
def bad_preprocessing(data):
    # MAJOR ISSUE: Fitting on full data before split!
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)  # üö® LEAKAGE!
    return train_test_split(scaled_data)

# ‚úÖ APPROVE - Proper data handling  
def good_preprocessing(data: pd.DataFrame) -> TrainTestSplit:
    # Split first, then fit only on training data
    train, test = train_test_split(data, test_size=0.2, random_state=42)
  
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train)      # ‚úÖ Fit on train
    test_scaled = scaler.transform(test)            # ‚úÖ Only transform test
  
    return TrainTestSplit(train_scaled, test_scaled, scaler)
```

#### **üîí 2. Security Issues (Auto-reject if found)**

```python
# ‚ùå REJECT - Security vulnerabilities
API_KEY = "sk-1234567890abcdef"  # üö® Hardcoded secret!
user_input = request.data
result = eval(user_input)        # üö® Code injection!

# ‚úÖ APPROVE - Secure implementation
class SecurePredictor:
    def __init__(self):
        self.api_key = os.getenv("API_KEY")  # ‚úÖ From environment
        self.input_validator = InputValidator()
  
    def predict(self, user_input: str) -> PredictionResult:
        # Validate and sanitize input
        clean_input = self.input_validator.sanitize(user_input)
        return self.model.predict(clean_input)
```

#### **üìù 3. Missing Type Hints (Request fix)**

```python
# ‚ùå NEEDS FIX - No type information
def train_model(data, config):
    return model.fit(data)

# ‚úÖ GOOD - Complete type information
def train_model(
    data: pd.DataFrame,
    config: TrainingConfig
) -> TrainedModel:
    """Train model with proper type safety."""
    return model.fit(data)
```

#### **üî• 4. Poor Error Handling (Request fix)**

```python
# ‚ùå NEEDS FIX - Silent failures
try:
    model = load_model(path)
    prediction = model.predict(data)
except:  # üö® Bare except!
    pass

# ‚úÖ GOOD - Specific error handling
try:
    model = load_model(path)
    prediction = model.predict(data)
except ModelLoadError as e:
    logger.error(f"Model loading failed: {e}")
    raise
except PredictionError as e:
    logger.error(f"Prediction failed: {e}")
    return None
```

---

### **Step 3: üß† AI/ML Logic Review (10 minutes)**

Focus on ML-specific correctness:

#### **üìä Data Pipeline Validation**

```python
# ‚úÖ EXCELLENT - Proper ML pipeline
class MLPipeline:
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.preprocessor = None
        self.model = None
        self.metrics_tracker = MetricsTracker()
  
    def train(self, data: pd.DataFrame) -> ModelResult:
        """Train with proper validation and tracking."""
        # 1. Split data properly
        train_data, val_data = self._split_data(data)
      
        # 2. Fit preprocessor ONLY on training data
        self.preprocessor = self._create_preprocessor()
        X_train = self.preprocessor.fit_transform(train_data.features)
        X_val = self.preprocessor.transform(val_data.features)  # Only transform!
      
        # 3. Train model
        self.model = self._create_model()
        self.model.fit(X_train, train_data.targets)
      
        # 4. Validate on unseen data
        val_predictions = self.model.predict(X_val)
        metrics = self._calculate_metrics(val_data.targets, val_predictions)
      
        # 5. Track experiment
        self.metrics_tracker.log_experiment(
            model_type=self.config.model_type,
            hyperparams=self.config.hyperparams,
            metrics=metrics
        )
      
        return ModelResult(
            model=self.model,
            preprocessor=self.preprocessor,
            metrics=metrics
        )
```

#### **üéØ Model Interface Validation**

```python
# ‚úÖ GOOD - Clear model interfaces
from abc import ABC, abstractmethod
from typing import Protocol

class MLModel(Protocol):
    """Modern protocol for ML models."""
  
    def fit(self, X: pd.DataFrame, y: pd.Series) -> Self: ...
    def predict(self, X: pd.DataFrame) -> np.ndarray: ...
    def get_feature_importance(self) -> dict[str, float]: ...

class RandomForestModel:
    """Concrete implementation following protocol."""
  
    def __init__(self, config: ModelConfig):
        self.model = RandomForestClassifier(**config.params)
        self.feature_names: list[str] = []
  
    def fit(self, X: pd.DataFrame, y: pd.Series) -> Self:
        self.feature_names = X.columns.tolist()
        self.model.fit(X, y)
        return self
  
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        if not self.feature_names:
            raise ModelNotTrainedError("Model not fitted yet")
        return self.model.predict(X)
  
    def get_feature_importance(self) -> dict[str, float]:
        return dict(zip(self.feature_names, self.model.feature_importances_))
```

---

### **Step 4: üèóÔ∏è Architecture Review (5 minutes)**

Quick architectural soundness check:

#### **‚úÖ Good Architecture Patterns**

```python
# ‚úÖ Dependency injection for testability
class ModelTrainer:
    def __init__(
        self,
        data_loader: DataLoader,
        preprocessor: DataPreprocessor,
        model_factory: ModelFactory,
        metrics_calculator: MetricsCalculator
    ):
        self.data_loader = data_loader
        self.preprocessor = preprocessor
        self.model_factory = model_factory
        self.metrics_calculator = metrics_calculator
  
    def train_model(self, config: TrainingConfig) -> TrainingResult:
        """Orchestrate training using injected dependencies."""
        data = self.data_loader.load(config.data_path)
        processed_data = self.preprocessor.process(data)
        model = self.model_factory.create(config.model_type)
      
        trained_model = model.fit(processed_data.X, processed_data.y)
        metrics = self.metrics_calculator.calculate(trained_model, processed_data)
      
        return TrainingResult(model=trained_model, metrics=metrics)

# ‚úÖ Configuration as first-class objects
@dataclass(frozen=True)
class ExperimentConfig:
    """Immutable experiment configuration."""
    model_type: str
    hyperparams: dict[str, Any]
    data_path: Path
    random_seed: int = 42
  
    def __post_init__(self):
        if self.random_seed < 0:
            raise ValueError("Random seed must be non-negative")

# ‚úÖ Async for I/O heavy operations
class AsyncDataProcessor:
    async def process_large_dataset(
        self,
        file_paths: list[Path],
        batch_size: int = 10000
    ) -> list[pd.DataFrame]:
        """Process multiple files concurrently."""
        async def process_file(path: Path) -> pd.DataFrame:
            return await self._process_single_file(path, batch_size)
      
        tasks = [process_file(path) for path in file_paths]
        return await asyncio.gather(*tasks)
```

---

## üìä **SIMPLE SCORING SYSTEM**

Instead of complex 100-point systems, use this **simple 3-tier approach**:

### **üü¢ APPROVED (Good to merge)**

- All automated checks pass ‚úÖ
- No data leakage or security issues ‚úÖ
- Type hints and error handling present ‚úÖ
- ML logic is sound ‚úÖ
- Architecture is reasonable ‚úÖ

### **üü° NEEDS FIXES (Fix then re-review)**

- Automated checks pass but has code quality issues
- Missing type hints or documentation
- Poor error handling
- Minor architectural concerns
- **Fix ‚Üí Quick re-review ‚Üí Approve**

### **üî¥ MAJOR REWORK (Start over)**

- Data leakage detected
- Security vulnerabilities
- Fundamentally broken architecture
- No tests for critical functionality
- **Needs design discussion before re-review**

---

## ü§ñ **AUTOMATED REVIEW ASSISTANT**

Use AI-powered review assistance:

```python
# review_assistant.py - AI-powered code review helper
class ReviewAssistant:
    """AI assistant for code reviews."""
  
    def analyze_ml_code(self, code_diff: str) -> ReviewSuggestions:
        """Analyze ML code changes for common issues."""
        suggestions = []
      
        # Check for data leakage patterns
        if self._detect_data_leakage(code_diff):
            suggestions.append(
                Suggestion(
                    type="critical",
                    message="Potential data leakage detected",
                    line_number=self._find_leakage_line(code_diff)
                )
            )
      
        # Check for missing type hints
        missing_types = self._find_missing_type_hints(code_diff)
        if missing_types:
            suggestions.append(
                Suggestion(
                    type="improvement", 
                    message=f"Add type hints to: {', '.join(missing_types)}"
                )
            )
      
        return ReviewSuggestions(suggestions)

    def _detect_data_leakage(self, code: str) -> bool:
        """Detect common data leakage patterns."""
        leakage_patterns = [
            r"\.fit_transform\(.*\).*train_test_split",  # fit_transform before split
            r"StandardScaler\(\)\.fit\(X\)",              # fit on full X
            r"\.fit\(.*data.*\).*\.split\(",              # fit before split
        ]
      
        return any(re.search(pattern, code) for pattern in leakage_patterns)
```

---

## üîß **MODERN REVIEW TOOLS**

```yaml
# .github/workflows/review.yml - Modern CI pipeline
name: AI/ML Code Review
on: [pull_request]

jobs:
  automated-review:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
    
      # Fast modern linting
      - name: Lint with Ruff
        run: ruff check src/ tests/
      
      # Type checking
      - name: Type check with mypy
        run: mypy src/ --strict
      
      # Security scan
      - name: Security scan
        run: bandit -r src/ -f json
      
      # ML-specific checks
      - name: Check for data leakage
        run: python scripts/ml_code_analyzer.py --check-leakage
      
      # Test with coverage
      - name: Test with pytest
        run: |
          pytest tests/ \
            --cov=src \
            --cov-fail-under=90 \
            --cov-report=xml
    
      # Generate review report
      - name: Generate AI review
        run: python scripts/review_assistant.py --diff ${{ github.event.pull_request.diff_url }}
```

---

## ‚úÖ **QUICK REVIEW CHECKLIST**

For reviewers - spend **max 20 minutes**:

### **‚ö° Fast Checks (2 min):**

- [ ] All CI checks are green
- [ ] No obvious security issues
- [ ] No data leakage patterns

### **üîç Code Quality (8 min):**

- [ ] Functions have type hints
- [ ] Error handling is specific
- [ ] Names are clear and ML-relevant
- [ ] No obvious performance issues

### **üß† ML Logic (8 min):**

- [ ] Data pipeline prevents leakage
- [ ] Model interfaces are clean
- [ ] Experiment tracking present
- [ ] Validation methodology sound

### **üìù Documentation (2 min):**

- [ ] Public functions have docstrings
- [ ] Breaking changes documented
- [ ] README updated if needed

---

## üí° **REVIEW BEST PRACTICES**

### **For Reviewers:**

- **Start with automated tools** - don't waste time on formatting
- **Focus on ML correctness** - data leakage is worse than style issues
- **Ask questions, don't just criticize** - "Why did you choose this approach?"
- **Approve fast when appropriate** - don't over-engineer simple fixes
- **Use examples** - show better alternatives, don't just say "fix this"

### **For Authors:**

- **Run tools locally first** - don't waste reviewer time
- **Write self-reviewing PR descriptions** - explain your choices
- **Test ML logic thoroughly** - unit test your data transformations
- **Keep PRs focused** - one feature/fix per PR
- **Respond to feedback quickly** - don't let reviews go stale

---

## üéØ **SUCCESS METRICS**

Track review effectiveness:

```python
# review_metrics.py
@dataclass
class ReviewMetrics:
    """Track code review effectiveness."""
    average_review_time: float  # Target: < 24 hours
    bugs_found_in_review: int   # Higher is better
    bugs_found_in_production: int  # Lower is better  
    review_feedback_score: float  # Team satisfaction
    automated_vs_manual_findings: float  # Automation effectiveness

def calculate_review_effectiveness(metrics: ReviewMetrics) -> float:
    """Calculate overall review process effectiveness."""
    # Fast reviews that catch bugs before production = effective
    speed_score = 1.0 if metrics.average_review_time < 24 else 0.5
    quality_score = metrics.bugs_found_in_review / max(1, metrics.bugs_found_in_production)
    automation_score = metrics.automated_vs_manual_findings
  
    return (speed_score + quality_score + automation_score) / 3
```

---

## üèÜ **REMEMBER**

> **"Perfect is the enemy of good. Ship working ML systems, improve iteratively."**

**Good code review:**

- **Catches critical issues** (security, data leakage)
- **Teaches the team** (better ML practices)
- **Ships features fast** (approves good-enough code)
- **Builds culture** (constructive, respectful feedback)

**The goal isn't perfect code - it's shipping reliable ML systems that solve real problems!** üöÄ
