# ü§ñ A.R.C.A LLM - Voice Conversational Assistant

**Advanced Reasoning Cognitive Architecture with Language Model and Memory**

Sistema de asistente conversacional por voz que usa:
- **Speech-to-Text**: Whisper Tiny (local, offline, 5x m√°s r√°pido)
- **LLM**: LM Studio con Qwen3-8B (m√°s r√°pido y moderno)
- **Text-to-Speech**: pyttsx3 (local, offline, r√°pido)
- **Memoria Conversacional**: Contexto completo durante la sesi√≥n

---

## üìã Caracter√≠sticas

‚úÖ **100% Offline** - Sin dependencias cloud, todo local  
‚úÖ **Memoria Conversacional** - Recuerda todo el contexto de la conversaci√≥n  
‚úÖ **Baja Latencia** - Optimizado para respuestas < 3 segundos  
‚úÖ **Interfaz Intuitiva** - Un solo bot√≥n para hablar  
‚úÖ **Domain-Driven Design** - Arquitectura limpia y mantenible  

---

## üöÄ Quick Start

### üê≥ Opci√≥n 1: Docker (Recomendado)

**M√°s f√°cil y sin problemas de dependencias:**

```bash
# 1. Aseg√∫rate de tener LM Studio corriendo con Qwen3-8B
# 2. Construir e iniciar
docker-compose up

# 3. Abrir http://localhost:8000
```

üìñ **Ver [docs/docker/DOCKER_SETUP.md](docs/docker/DOCKER_SETUP.md) para documentaci√≥n completa**

---

### üíª Opci√≥n 2: Instalaci√≥n Local

**Prerequisitos:**
- **Python 3.11+**
- **LM Studio** instalado y corriendo en `http://127.0.0.1:1234`
- **Modelo cargado** en LM Studio: `qwen/qwen3-8b`

### 2. Instalaci√≥n

```bash
# Clonar repositorio
cd A.R.C.A-LLM

# Crear virtual environment
python -m venv arca-venv
source arca-venv/bin/activate  # En Windows: arca-venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 3. Configuraci√≥n (Opcional)

El sistema funciona con defaults razonables sin `.env`. Solo cr√©alo si necesitas personalizar:

```bash
# OPCIONAL: Solo si quieres cambiar configuraci√≥n
cp .env.example .env
# Luego edita .env con tus valores
```

**Defaults autom√°ticos (sin .env):**
- LM Studio: `http://127.0.0.1:1234`
- Whisper: modelo `base`, CPU, int8
- TTS: rate 175, volume 0.9
- API: puerto 8000

**Cu√°ndo necesitas .env:**
- Usar otro modelo LLM
- Cambiar Whisper a GPU (cuda)
- Usar modelo Whisper m√°s r√°pido (tiny)
- Cambiar puerto de API

### 4. Iniciar LM Studio

1. Abrir LM Studio
2. Cargar modelo: `openai-gpt-oss-20b-abliterated-uncensored-neo-imatrix`
3. Iniciar servidor local (asegurarse que corre en port 1234)

### 5. Ejecutar A.R.C.A

```bash
# Desde la ra√≠z del proyecto
python -m src.api.main

# O con uvicorn directamente
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
```

### 6. Usar la Aplicaci√≥n

Abrir navegador en: **http://localhost:8000**

1. Presionar el bot√≥n del micr√≥fono üé§
2. Hablar (el bot√≥n se pone rojo)
3. Click de nuevo para enviar
4. Esperar respuesta (autom√°ticamente se reproduce)
5. ¬°Repetir!

---

## üîå API para Integraci√≥n Frontend

üìñ **Documentaci√≥n completa**: [docs/API_DOCUMENTATION.md](docs/API_DOCUMENTATION.md)

### Endpoint Principal

```http
POST /api/voice/process
Content-Type: multipart/form-data

Body:
- audio: File (audio blob)
- conversation_id: String (optional)

Response:
- Body: audio/wav (respuesta en audio)
- Headers:
  - X-Conversation-Id (Base64)
  - X-Transcribed-Text (Base64)
  - X-Response-Text (Base64)
```

### Ejemplo de Integraci√≥n

```javascript
const formData = new FormData();
formData.append('audio', audioBlob, 'voice.webm');

const response = await fetch('http://localhost:8000/api/voice/process', {
  method: 'POST',
  body: formData
});

// Decodificar headers
const conversationId = atob(response.headers.get('X-Conversation-Id'));
const transcribedText = atob(response.headers.get('X-Transcribed-Text'));
const llmResponse = atob(response.headers.get('X-Response-Text'));

// Reproducir audio
const audio = new Audio(URL.createObjectURL(await response.blob()));
audio.play();
```

**Otros Endpoints**:
- `GET /api/health` - Health check
- `GET /api/voice/conversation/{id}` - Obtener historial
- `DELETE /api/voice/conversation/{id}` - Eliminar conversaci√≥n
- `WS /ws/voice` - WebSocket para streaming real-time

---

## üìÇ Arquitectura

### Estructura del Proyecto

```
A.R.C.A-LLM/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ domain/                 # Rich Domain Models (DDD)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation.py    # Aggregate Root
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ message.py         # Value Object
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ application/            # Application Services
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ conversation_service.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ voice_assistant_service.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ infrastructure/         # Technical Implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ lm_studio_client.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ stt/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ whisper_client.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tts/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ pyttsx3_client.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ api/                    # Presentation Layer (FastAPI)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ voice_routes.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ frontend/               # Web Interface
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ static/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ style.css
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ js/
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ voice-interface.js
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ config.py               # Configuration Management
‚îÇ
‚îú‚îÄ‚îÄ tests/                      # Unit & Integration Tests
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îú‚îÄ‚îÄ diagrams/                   # Architecture Diagrams
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

### Pipeline de Procesamiento

```
[Usuario habla] 
    ‚Üì
[MediaRecorder captura audio]
    ‚Üì
[POST /api/voice/process]
    ‚Üì
[Whisper STT] ‚Üí Texto transcrito
    ‚Üì
[Conversation + LLM] ‚Üí Respuesta con memoria
    ‚Üì
[pyttsx3 TTS] ‚Üí Audio sint√©tico
    ‚Üì
[Response con audio bytes]
    ‚Üì
[Auto-reproducci√≥n en navegador]
```

---

## üéØ API Endpoints

### `POST /api/voice/process`
Procesar audio de voz y retornar respuesta.

**Request:**
- `audio`: File (WAV, WEBM, MP3)
- `session_id`: String (optional, UUID)
- `language`: String (default: "es")

**Response:**
- Audio WAV (binary)
- Headers con transcripci√≥n, respuesta, latencias

### `POST /api/text/process`
Procesar texto sin audio (para testing).

**Request:**
```json
{
  "text": "Hola, c√≥mo est√°s?",
  "session_id": "optional-uuid"
}
```

**Response:**
```json
{
  "session_id": "uuid",
  "response_text": "Hola! Muy bien...",
  "latency": {"llm": 1.2, "tts": 0.4, "total": 1.6}
}
```

### `GET /api/conversation/{session_id}`
Obtener historial completo de conversaci√≥n.

### `DELETE /api/conversation/{session_id}`
Limpiar historial de conversaci√≥n.

### `GET /health`
Health check de todos los componentes.

---

## ‚öôÔ∏è Configuraci√≥n Avanzada

### Optimizar Latencia

**Whisper m√°s r√°pido:**
```env
WHISPER_MODEL=tiny  # Menos preciso pero 3x m√°s r√°pido
```

**LLM respuestas m√°s cortas:**
```env
LLM_MAX_TOKENS=100  # Respuestas m√°s concisas
LLM_TEMPERATURE=0.5  # Menos creativo, m√°s directo
```

### Usar GPU (si disponible)

```env
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16
```

---

## üß™ Testing

```bash
# Ejecutar tests
pytest tests/ -v

# Con coverage
pytest tests/ --cov=src --cov-report=html

# Test espec√≠fico
pytest tests/test_conversation_memory.py -v
```

---

## üìä Ejemplo de Conversaci√≥n

```
Usuario: "Hola, c√≥mo est√°s? Me llamo Adrian"
A.R.C.A: "Hola Adrian! Muy bien, gracias. ¬øEn qu√© puedo ayudarte hoy?"

Usuario: "Qu√© d√≠a es hoy?"
A.R.C.A: "Hoy es viernes 31 de octubre de 2025."

Usuario: "Recuerdas mi nombre?"
A.R.C.A: "S√≠, claro! Te llamas Adrian. ¬øHay algo m√°s en lo que pueda ayudarte?"
```

---

## üîß Troubleshooting

### Error: "LM Studio connection refused"
- Verificar que LM Studio est√° corriendo
- Verificar puerto 1234 est√° libre
- Verificar modelo est√° cargado

### Error: "Microphone access denied"
- Dar permisos de micr√≥fono al navegador
- HTTPS requerido (o localhost)

### Audio muy rob√≥tico
- Ajustar `TTS_RATE` (150-200)
- Considerar upgrade a Coqui TTS (futuro)

### Latencia alta
- Usar `WHISPER_MODEL=tiny`
- Reducir `LLM_MAX_TOKENS`
- Verificar CPU/GPU disponible

---

## üöß Roadmap

- [ ] WebSocket streaming para latencia ultra-baja
- [ ] Persistencia de conversaciones (SQLite)
- [ ] M√∫ltiples voces TTS
- [ ] Upgrade a Coqui TTS para mejor calidad
- [ ] Multi-idioma UI
- [ ] Docker deployment
- [ ] An√°lisis de sentimiento

---

## üìÑ Licencia

MIT License - Ver LICENSE file

---

## üë®‚Äçüíª Autor

Proyecto A.R.C.A LLM - Advanced Voice Conversational Assistant

---

## üôè Agradecimientos

- **Whisper** (OpenAI) - Speech-to-Text
- **LM Studio** - Local LLM serving
- **FastAPI** - Modern web framework
- **pyttsx3** - Text-to-Speech

---

**¬øListo para hablar con A.R.C.A? üé§ü§ñ**

