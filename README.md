# ðŸ¤– A.R.C.A LLM - Voice Conversational Assistant

**Advanced Reasoning Cognitive Architecture with Language Model and Memory**

Sistema de asistente conversacional por voz que usa:
- **Speech-to-Text**: Whisper Tiny (local, offline, 5x mÃ¡s rÃ¡pido)
- **LLM**: LM Studio con Qwen3-4B-2507 (rÃ¡pido y optimizado)
- **Text-to-Speech**: pyttsx3 (local, offline, rÃ¡pido)
- **Memoria Conversacional**: Contexto completo durante la sesiÃ³n

---

## ðŸ“‹ CaracterÃ­sticas

âœ… **100% Offline** - Sin dependencias cloud, todo local  
âœ… **Memoria Conversacional** - Recuerda todo el contexto de la conversaciÃ³n  
âœ… **Baja Latencia** - Optimizado para respuestas < 3 segundos  
âœ… **Interfaz Intuitiva** - Un solo botÃ³n para hablar  
âœ… **Domain-Driven Design** - Arquitectura limpia y mantenible  

---

## ðŸš€ Quick Start

### ðŸŽ¨ OpciÃ³n 1: Interfaz Tkinter con Orbe Jarvis (Desktop) â­ NUEVO

**Interfaz desktop futurista con orbe animado estilo Jarvis/Iron Man:**

```bash
# Ejecutar interfaz Tkinter
python -m src.frontend_tkinter.orbe_window

# O con el resto del sistema:
# (por implementar en TICKET-005)
```

**CaracterÃ­sticas:**
- âœ¨ Orbe animado estilo Jarvis con efectos glow
- ðŸŽ¤ Click en orbe para activar voz
- ðŸŒˆ Estados visuales (idle, listening, processing, speaking)
- ðŸ–¥ï¸ Ventana siempre al frente
- âŒ¨ï¸ Esc o Click derecho para salir

**Status:** ðŸ”„ En desarrollo (Branch: `frontendTkinter`)

---

### ðŸ³ OpciÃ³n 2: Docker (Web Interface)

**MÃ¡s fÃ¡cil y sin problemas de dependencias:**

```bash
# 1. AsegÃºrate de tener LM Studio corriendo con Qwen3-8B
# 2. Construir e iniciar
docker-compose up

# 3. Abrir http://localhost:8000
```

ðŸ“– **Ver [docs/docker/DOCKER_SETUP.md](docs/docker/DOCKER_SETUP.md) para documentaciÃ³n completa**

---

### ðŸ’» OpciÃ³n 3: InstalaciÃ³n Local (Web Interface)

**Prerequisitos:**
- **Python 3.11+**
- **LM Studio** instalado y corriendo en `http://192.168.1.38:1234`
- **Modelo cargado** en LM Studio: `qwen/qwen3-4b-2507`

### 2. InstalaciÃ³n

```bash
# Clonar repositorio
cd A.R.C.A-LLM

# Crear virtual environment
python -m venv arca-venv
source arca-venv/bin/activate  # En Windows: arca-venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 3. ConfiguraciÃ³n

**El sistema funciona inmediatamente con defaults optimizados:**
- LM Studio: `http://192.168.1.38:1234/v1`
- Modelo LLM: `qwen/qwen3-4b-2507`
- Whisper: modelo `tiny`, CPU, int8
- TTS: rate 175, volume 0.9
- API: puerto 8000

**Para cambiar configuraciÃ³n (opcional):**

Crear archivo `.env` en la raÃ­z del proyecto:
```bash
# .env
LM_STUDIO_URL=http://otra-ip:1234/v1
LM_STUDIO_MODEL=otro-modelo
WHISPER_MODEL=base
```

Ver todas las variables disponibles en `src/config.py`

### 4. Iniciar LM Studio

1. Abrir LM Studio
2. Cargar modelo: `qwen/qwen3-4b-2507` (o cualquier modelo compatible)
3. Iniciar servidor local en el puerto 1234
4. Verificar que la URL del servidor sea accesible: `http://192.168.1.38:1234`

**Los defaults ya estÃ¡n configurados correctamente en `src/config.py`**

Si necesitas cambiar la configuraciÃ³n, crea un archivo `.env`:
```bash
# .env (opcional)
LM_STUDIO_URL=http://192.168.1.38:1234/v1
LM_STUDIO_MODEL=qwen/qwen3-4b-2507
WHISPER_MODEL=tiny
```

### 5. Ejecutar A.R.C.A

```bash
# Desde la raÃ­z del proyecto
python -m src.api.main

# O con uvicorn directamente
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 --reload
```

### 6. Usar la AplicaciÃ³n

Abrir navegador en: **http://localhost:8000**

1. Presionar el botÃ³n del micrÃ³fono ðŸŽ¤
2. Hablar (el botÃ³n se pone rojo)
3. Click de nuevo para enviar
4. Esperar respuesta (automÃ¡ticamente se reproduce)
5. Â¡Repetir!

---

## ðŸ”Œ API para IntegraciÃ³n Frontend

ðŸ“– **DocumentaciÃ³n completa**: [docs/API_DOCUMENTATION.md](docs/API_DOCUMENTATION.md)

### Endpoint Principal

```http
POST /api/voice/process
Content-Type: multipart/form-data

Body:
- audio: File (audio blob)
- conversation_id: String (optional)

Response:
- Body: audio/wav (respuesta en audio)
- Headers:
  - X-Conversation-Id (Base64)
  - X-Transcribed-Text (Base64)
  - X-Response-Text (Base64)
```

### Ejemplo de IntegraciÃ³n

```javascript
const formData = new FormData();
formData.append('audio', audioBlob, 'voice.webm');

const response = await fetch('http://localhost:8000/api/voice/process', {
  method: 'POST',
  body: formData
});

// Decodificar headers
const conversationId = atob(response.headers.get('X-Conversation-Id'));
const transcribedText = atob(response.headers.get('X-Transcribed-Text'));
const llmResponse = atob(response.headers.get('X-Response-Text'));

// Reproducir audio
const audio = new Audio(URL.createObjectURL(await response.blob()));
audio.play();
```

**Otros Endpoints**:
- `GET /api/health` - Health check
- `GET /api/voice/conversation/{id}` - Obtener historial
- `DELETE /api/voice/conversation/{id}` - Eliminar conversaciÃ³n
- `WS /ws/voice` - WebSocket para streaming real-time

---

## ðŸ“‚ Arquitectura

### Estructura del Proyecto

```
A.R.C.A-LLM/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ domain/                 # Rich Domain Models (DDD)
â”‚   â”‚   â”œâ”€â”€ conversation.py    # Aggregate Root
â”‚   â”‚   â””â”€â”€ message.py         # Value Object
â”‚   â”‚
â”‚   â”œâ”€â”€ application/            # Application Services
â”‚   â”‚   â”œâ”€â”€ conversation_service.py
â”‚   â”‚   â””â”€â”€ voice_assistant_service.py
â”‚   â”‚
â”‚   â”œâ”€â”€ infrastructure/         # Technical Implementations
â”‚   â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”‚   â””â”€â”€ lm_studio_client.py
â”‚   â”‚   â”œâ”€â”€ stt/
â”‚   â”‚   â”‚   â””â”€â”€ whisper_client.py
â”‚   â”‚   â””â”€â”€ tts/
â”‚   â”‚       â””â”€â”€ pyttsx3_client.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                    # Presentation Layer (FastAPI)
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â””â”€â”€ voice_routes.py
â”‚   â”‚
â”‚   â”œâ”€â”€ frontend/               # Web Interface
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â””â”€â”€ index.html
â”‚   â”‚   â””â”€â”€ static/
â”‚   â”‚       â”œâ”€â”€ css/
â”‚   â”‚       â”‚   â””â”€â”€ style.css
â”‚   â”‚       â””â”€â”€ js/
â”‚   â”‚           â””â”€â”€ voice-interface.js
â”‚   â”‚
â”‚   â””â”€â”€ config.py               # Configuration Management
â”‚
â”œâ”€â”€ tests/                      # Unit & Integration Tests
â”œâ”€â”€ docs/                       # Documentation
â”œâ”€â”€ diagrams/                   # Architecture Diagrams
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### Pipeline de Procesamiento

```
[Usuario habla] 
    â†“
[MediaRecorder captura audio]
    â†“
[POST /api/voice/process]
    â†“
[Whisper STT] â†’ Texto transcrito
    â†“
[Conversation + LLM] â†’ Respuesta con memoria
    â†“
[pyttsx3 TTS] â†’ Audio sintÃ©tico
    â†“
[Response con audio bytes]
    â†“
[Auto-reproducciÃ³n en navegador]
```

---

## ðŸŽ¯ API Endpoints

### `POST /api/voice/process`
Procesar audio de voz y retornar respuesta.

**Request:**
- `audio`: File (WAV, WEBM, MP3)
- `session_id`: String (optional, UUID)
- `language`: String (default: "es")

**Response:**
- Audio WAV (binary)
- Headers con transcripciÃ³n, respuesta, latencias

### `POST /api/text/process`
Procesar texto sin audio (para testing).

**Request:**
```json
{
  "text": "Hola, cÃ³mo estÃ¡s?",
  "session_id": "optional-uuid"
}
```

**Response:**
```json
{
  "session_id": "uuid",
  "response_text": "Hola! Muy bien...",
  "latency": {"llm": 1.2, "tts": 0.4, "total": 1.6}
}
```

### `GET /api/conversation/{session_id}`
Obtener historial completo de conversaciÃ³n.

### `DELETE /api/conversation/{session_id}`
Limpiar historial de conversaciÃ³n.

### `GET /health`
Health check de todos los componentes.

---

## âš™ï¸ ConfiguraciÃ³n Avanzada

### Optimizar Latencia

**Whisper mÃ¡s rÃ¡pido:**
```env
WHISPER_MODEL=tiny  # Menos preciso pero 3x mÃ¡s rÃ¡pido
```

**LLM respuestas mÃ¡s cortas:**
```env
LLM_MAX_TOKENS=100  # Respuestas mÃ¡s concisas
LLM_TEMPERATURE=0.5  # Menos creativo, mÃ¡s directo
```

### Usar GPU (si disponible)

```env
WHISPER_DEVICE=cuda
WHISPER_COMPUTE_TYPE=float16
```

---

## ðŸ§ª Testing

```bash
# Ejecutar tests
pytest tests/ -v

# Con coverage
pytest tests/ --cov=src --cov-report=html

# Test especÃ­fico
pytest tests/test_conversation_memory.py -v
```

---

## ðŸ“Š Ejemplo de ConversaciÃ³n

```
Usuario: "Hola, cÃ³mo estÃ¡s? Me llamo Adrian"
A.R.C.A: "Hola Adrian! Muy bien, gracias. Â¿En quÃ© puedo ayudarte hoy?"

Usuario: "QuÃ© dÃ­a es hoy?"
A.R.C.A: "Hoy es viernes 31 de octubre de 2025."

Usuario: "Recuerdas mi nombre?"
A.R.C.A: "SÃ­, claro! Te llamas Adrian. Â¿Hay algo mÃ¡s en lo que pueda ayudarte?"
```

---

## ðŸ”§ Troubleshooting

### Error: "LM Studio connection refused"
- Verificar que LM Studio estÃ¡ corriendo
- Verificar puerto 1234 estÃ¡ libre
- Verificar modelo estÃ¡ cargado

### Error: "Microphone access denied"
- Dar permisos de micrÃ³fono al navegador
- HTTPS requerido (o localhost)

### Audio muy robÃ³tico
- Ajustar `TTS_RATE` (150-200)
- Considerar upgrade a Coqui TTS (futuro)

### Latencia alta
- Usar `WHISPER_MODEL=tiny`
- Reducir `LLM_MAX_TOKENS`
- Verificar CPU/GPU disponible

---

## ðŸš§ Roadmap

- [ ] WebSocket streaming para latencia ultra-baja
- [ ] Persistencia de conversaciones (SQLite)
- [ ] MÃºltiples voces TTS
- [ ] Upgrade a Coqui TTS para mejor calidad
- [ ] Multi-idioma UI
- [ ] Docker deployment
- [ ] AnÃ¡lisis de sentimiento

---

## ðŸ“„ Licencia

MIT License - Ver LICENSE file

---

## ðŸ‘¨â€ðŸ’» Autor

Proyecto A.R.C.A LLM - Advanced Voice Conversational Assistant

---

## ðŸ™ Agradecimientos

- **Whisper** (OpenAI) - Speech-to-Text
- **LM Studio** - Local LLM serving
- **FastAPI** - Modern web framework
- **pyttsx3** - Text-to-Speech

---

**Â¿Listo para hablar con A.R.C.A? ðŸŽ¤ðŸ¤–**

