"""
LMStudioClient - Cliente para LLM local vÃ­a LM Studio.

Usa OpenAI SDK para compatibilidad con LM Studio.
"""

import asyncio
from typing import Optional
from openai import AsyncOpenAI, OpenAIError
from loguru import logger


class LMStudioClient:
    """
    Cliente para LM Studio usando protocolo compatible con OpenAI.
    
    LM Studio expone una API compatible con OpenAI que permite usar
    el SDK oficial para comunicaciÃ³n con modelos locales.
    """
    
    def __init__(
        self,
        base_url: str,
        model: str,
        max_tokens: int,
        temperature: float
    ):
        """
        Inicializar cliente LM Studio.
        
        Args:
            base_url: URL del servidor LM Studio
            model: Nombre del modelo en LM Studio
            max_tokens: LÃ­mite de tokens para respuestas
            temperature: Creatividad del modelo (0.0-2.0)
        
        Note: Valores vienen de config.py (Ãºnica fuente de verdad)
        """
        self.base_url = base_url
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        
        # Cliente OpenAI configurado para LM Studio
        self.client = AsyncOpenAI(
            base_url=base_url,
            api_key="not-needed"  # LM Studio no requiere API key
        )
        
        logger.info(
            f"ðŸ§  LMStudio client initialized: {base_url}, "
            f"model={model}, max_tokens={max_tokens}"
        )
    
    async def generate_response(
        self,
        messages: list[dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ) -> str:
        """
        Generar respuesta usando el LLM local.
        
        Args:
            messages: Lista de mensajes en formato OpenAI
                     [{"role": "user"|"assistant"|"system", "content": "..."}]
            max_tokens: Override de lÃ­mite de tokens
            temperature: Override de temperatura
            
        Returns:
            Texto de la respuesta generada
            
        Raises:
            ValueError: Si messages estÃ¡ vacÃ­o
            RuntimeError: Si LM Studio no responde
        """
        if not messages:
            raise ValueError("Messages list cannot be empty")
        
        # Usar valores override o defaults
        tokens = max_tokens or self.max_tokens
        temp = temperature or self.temperature
        
        logger.info(
            f"ðŸ¤– Generating response: {len(messages)} messages, "
            f"max_tokens={tokens}, temp={temp}"
        )
        
        try:
            # Llamada async al LLM
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=tokens,
                temperature=temp,
                stream=False  # Sin streaming por ahora (optimizaciÃ³n futura)
            )
            
            # Extraer texto de respuesta
            message = response.choices[0].message
            content = message.content
            
            # Algunos modelos (como QwQ/reasoning models) usan reasoning_content
            # Si content estÃ¡ vacÃ­o, intentar con reasoning_content
            if not content or not content.strip():
                reasoning_content = getattr(message, 'reasoning_content', None)
                if reasoning_content and reasoning_content.strip():
                    logger.debug("Using reasoning_content instead of content")
                    content = reasoning_content
            
            # Validar que el contenido no sea None o vacÃ­o
            if content is None:
                logger.error("âŒ LLM returned None content")
                logger.debug(f"Raw response: {response}")
                raise RuntimeError("LLM returned empty response (None)")
            
            response_text = content.strip()
            
            if not response_text:
                logger.error("âŒ LLM returned empty content after strip")
                logger.debug(f"Raw response: {response}")
                raise RuntimeError("LLM returned empty response")
            
            logger.info(f"âœ… Response generated: '{response_text[:50]}...'")
            return response_text
            
        except OpenAIError as e:
            logger.error(f"âŒ LM Studio API error: {e}")
            raise RuntimeError(
                f"LM Studio failed to respond. "
                f"Ensure LM Studio is running at {self.base_url}"
            ) from e
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            raise RuntimeError(f"LLM generation error: {e}") from e
    
    async def generate_response_stream(
        self,
        messages: list[dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None
    ):
        """
        Generar respuesta en modo streaming (para optimizaciÃ³n futura).
        
        Args:
            messages: Lista de mensajes
            max_tokens: LÃ­mite de tokens
            temperature: Temperatura
            
        Yields:
            Chunks de texto conforme se generan
        """
        tokens = max_tokens or self.max_tokens
        temp = temperature or self.temperature
        
        try:
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=tokens,
                temperature=temp,
                stream=True
            )
            
            async for chunk in stream:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error(f"âŒ Streaming error: {e}")
            raise RuntimeError(f"LLM streaming error: {e}") from e
    
    async def health_check(self) -> bool:
        """
        Verificar que LM Studio estÃ¡ accesible.
        
        Returns:
            True si LM Studio responde, False si no
        """
        try:
            # Mensaje simple de prueba
            test_messages = [
                {"role": "user", "content": "Hi"}
            ]
            
            response = await asyncio.wait_for(
                self.generate_response(test_messages, max_tokens=5),
                timeout=10.0
            )
            
            logger.info("âœ… LM Studio health check passed")
            return True
            
        except asyncio.TimeoutError:
            logger.warning("â±ï¸ LM Studio health check timeout")
            return False
        except Exception as e:
            logger.warning(f"âš ï¸ LM Studio health check failed: {e}")
            return False
    
    def cleanup(self) -> None:
        """Limpiar recursos."""
        logger.info("ðŸ§¹ Cleaning up LMStudio client")
        # AsyncOpenAI maneja su propio cleanup

